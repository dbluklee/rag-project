"""
CHEESEADE RAG Server - ë©”ì¸ ì„œë²„
ì£¼ìš” ê¸°ëŠ¥: í™˜ê²½ì„¤ì •, ëª¨ë¸ ì´ˆê¸°í™”, ì²­í‚¹, ì„ë² ë”©, ë¦¬íŠ¸ë¦¬ë²„, RAG êµ¬ì„±, FastAPI ì‹¤í–‰
"""
import os
import uvicorn
import requests
import torch

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.runnables import RunnableParallel

from chunking.chunking_md import chunk_markdown_file
from embedding.bge_m3 import get_bge_m3_model
from retriever.retriever import get_retriever
from vector_db.milvus import MilvusVectorStore

# API ëª¨ë“ˆë“¤ import
from api.routes import router as api_router, set_chat_handler
from api.chat_handler import ChatHandler

# ================================
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
# ================================

print("ğŸ”§ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
LLM_SERVER_URL = os.environ["LLM_SERVER_URL"]
RAG_MODEL_NAME = os.environ["RAG_MODEL_NAME"]
MILVUS_SERVER_IP = os.environ["MILVUS_SERVER_IP"]
MILVUS_PORT = os.environ["MILVUS_PORT"]
LLM_MODEL_NAME = os.environ["LLM_MODEL_NAME"]
collection_name = os.environ["COMPANY_NAME"].lower+'_'+os.environ["METRIC_TYPE"].lower+'_'+os.environ["INDEX_TYPE"].lower

print(f"âœ… í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ")
print(f"   LLM ì„œë²„: {LLM_SERVER_URL}")
print(f"   RAG ëª¨ë¸: {RAG_MODEL_NAME}")
print(f"   LLM ëª¨ë¸: {LLM_MODEL_NAME}")
print(f"   Milvus: {MILVUS_SERVER_IP}:{MILVUS_PORT}")
print(f"   ì»¬ë ‰ì…˜: {collection_name}")

# ================================
# LLM ì„œë²„ ì—°ê²° ë° ì´ˆê¸°í™”
# ================================

print(f"\nğŸ”— LLM ì„œë²„ ì—°ê²° ì‹œë„...")
try:
    response = requests.get(f"{LLM_SERVER_URL}/api/tags", timeout=10)
    if response.status_code == 200:
        print(f"âœ… LLM ì„œë²„ ì—°ê²° ì„±ê³µ: {LLM_SERVER_URL}")
    else:
        print(f"âš ï¸ LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        
    llm = ChatOllama(
        model=LLM_MODEL_NAME,
        base_url=LLM_SERVER_URL,
        timeout=120
    )
    print(f"âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ: {LLM_MODEL_NAME}")
    
except requests.exceptions.ConnectionError:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {LLM_SERVER_URL}")
    llm = None
except Exception as e:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
    llm = None

# ================================
# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# ================================

print(f"\nğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
if torch.cuda.is_available():
    print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    device = 'cuda'
else:
    print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš©")
    device = 'cpu'

print(f"ğŸ”§ {device}ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
embedding_model = get_bge_m3_model()
print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ================================
# ë¬¸ì„œ ì²­í‚¹
# ================================

print(f"\nğŸ“ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘...")
markdown_path = "./docs/feature.md"
chunks = chunk_markdown_file(markdown_path)

if len(chunks) == 0:
    print("âŒ ë¬¸ì„œ ì²­í‚¹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
    raise ValueError("ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨ - ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤")

print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")

# ================================
# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì¶”ê°€
# ================================

print(f"\nğŸ—„ï¸ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”...")
vector_store = MilvusVectorStore(
    collection_name=collection_name, 
    embedding_model=embedding_model,
    metric_type=METRIC_TYPE,
    index_type=INDEX_TYPE,
    milvus_host=MILVUS_SERVER_IP,  
    milvus_port=MILVUS_PORT   
)

print(f"\nğŸ“¤ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€...")
inserted_ids = vector_store.add_documents(chunks)
print(f"âœ… ë²¡í„° DB ì¶”ê°€ ì™„ë£Œ: {len(inserted_ids)}ê°œ ë¬¸ì„œ")

# ================================
# ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
# ================================

print(f"\nğŸ” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±...")
retriever = get_retriever(vector_store, retriever_type='top_k')
print(f"âœ… ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì™„ë£Œ")

# ================================
# RAG ì²´ì¸ êµ¬ì„±
# ================================

print(f"\nğŸ”— RAG ì²´ì¸ êµ¬ì„±...")

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
system_prompt = '''Answer the user's Question from the Context.
Keep your answer ground in the facts of the Context.
If the Context doesn't contain the facts to answer, just output 'ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
Please answer in Korean.'''

# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
RAG_prompt = ChatPromptTemplate([
    ('system', system_prompt),
    ('user', '''Context: {context}
    ---
     Question: {question}''')
])

# RAG ì²´ì¸ êµ¬ì„±
rag_chain = (
    RunnableParallel(
        context=retriever, 
        question=RunnablePassthrough()
    )
    | RAG_prompt
    | llm
    | StrOutputParser()
)

print(f"âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ")

# ================================
# ì±„íŒ… í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
# ================================

print(f"\nğŸ’¬ ì±„íŒ… í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”...")
chat_handler = ChatHandler(
    rag_chain=rag_chain,
    retriever=retriever,
    rag_model_name=RAG_MODEL_NAME,
    llm_server_url=LLM_SERVER_URL
)

# API ë¼ìš°í„°ì— ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì •
set_chat_handler(chat_handler)
print(f"âœ… ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")

# ================================
# FastAPI ì•± ìƒì„± ë° ì„¤ì •
# ================================

print(f"\nğŸš€ FastAPI ì•± ìƒì„±...")
app = FastAPI(
    title="CHEESEADE RAG Server", 
    description="RAG API ì„œë²„ with OpenWebUI í˜¸í™˜", 
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# API ë¼ìš°í„° ë“±ë¡
app.include_router(api_router)

print(f"âœ… FastAPI ì„¤ì • ì™„ë£Œ")

# ================================
# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ================================

@app.get("/")
async def root():
    """ì‹œìŠ¤í…œ ì •ë³´ ë° ìƒíƒœ"""
    return {
        "message": "CHEESEADE RAG Server",
        "version": "1.0.0",
        "status": "running",
        "system": {
            "llm_model": LLM_MODEL_NAME,
            "rag_model": RAG_MODEL_NAME,
            "embedding_device": device,
            "documents_loaded": len(chunks),
            "vector_collection": collection_name
        },
        "endpoints": {
            "chat": "/api/chat/completions",
            "models": "/api/models", 
            "tags": "/api/tags",
            "health": "/health",
            "debug": "/debug/test-retrieval"
        },
        "features": [
            "RAG (Retrieval-Augmented Generation)",
            "OpenAI Compatible API",
            "Ollama Compatible API", 
            "OpenWebUI Compatible API",
            "Streaming Support"
        ]
    }

# ================================
# ì„œë²„ ì‹¤í–‰
# ================================

if __name__ == "__main__":
    print(f"\nğŸ¯ ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì‹œìŠ¤í…œ ìš”ì•½:")
    print(f"   ğŸ¤– LLM ëª¨ë¸: {LLM_MODEL_NAME}")
    print(f"   ğŸ” RAG ëª¨ë¸: {RAG_MODEL_NAME}")
    print(f"   ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ: {len(chunks)}ê°œ")
    print(f"   ğŸ’¾ ë²¡í„° ì»¬ë ‰ì…˜: {collection_name}")
    print(f"   ğŸ–¥ï¸ ì„ë² ë”© ë””ë°”ì´ìŠ¤: {device}")
    print(f"\nğŸŒ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:8000")
    print(f"ğŸ“– API ë¬¸ì„œ: http://0.0.0.0:8000/docs")
    print