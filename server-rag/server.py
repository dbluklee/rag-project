import os
import uvicorn
import uuid
import time
import json
import requests
import torch
import asyncio

from typing import List, Optional
from fastapi import FastAPI, HTTPException, Request, Header
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.runnables import RunnableParallel

from chunking.chunking_md import chunk_markdown_file
from embedding.bge_m3 import get_bge_m3_model
from retriever.retriever import get_retriever
from vector_db.milvus import MilvusVectorStore


# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
LLM_SERVER_URL = os.environ["LLM_SERVER_URL"]
RAG_MODEL_NAME = os.environ["RAG_MODEL_NAME"]
MILVUS_SERVER_IP = os.environ["MILVUS_SERVER_IP"]
MILVUS_PORT = os.environ["MILVUS_PORT"]
LLM_MODEL_NAME = os.environ["LLM_MODEL_NAME"]
COLLECTION_NAME = os.environ["COLLECTION_NAME"]

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
system_prompt = '''Answer the user's Question from the Context.
Keep your answer ground in the facts of the Context.
If the Context doesn't contain the facts to answer, just output 'ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
Please answer in Korean.'''

# LLM ì„œë²„ì— ì—°ê²°
try:
    print(f"ğŸ”— LLM ì„œë²„ ì—°ê²° ì‹œë„: {LLM_SERVER_URL}")
    response = requests.get(f"{LLM_SERVER_URL}/api/tags", timeout=10)
    if response.status_code == 200:
        print(f"âœ… LLM ì„œë²„ ì—°ê²° ì„±ê³µ: {LLM_SERVER_URL}")
    else:
        print(f"âš ï¸ LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        
    llm = ChatOllama(
        model=LLM_MODEL_NAME,
        base_url=LLM_SERVER_URL,
        timeout=120
    )
    print(f"âœ… LLM ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    
except requests.exceptions.ConnectionError:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {LLM_SERVER_URL}")
    llm = None
except Exception as e:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
    llm = None
 
# RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
RAG_prompt = ChatPromptTemplate([
    ('system', system_prompt),
    ('user', '''Context: {context}
    ---
     Question: {question}''')
])





# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU ìš°ì„  ì‚¬ìš©)
if torch.cuda.is_available():
    print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    device = 'cuda'
else:
    print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš©")
    device = 'cpu'

print(f"ğŸ”§ {device}ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„")
embedding_model = get_bge_m3_model()
print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")





# 3. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
markdown_path = "./docs/feature.md"

print(f"\nğŸ“ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘...")
chunks = chunk_markdown_file(markdown_path)

if len(chunks) == 0:
    print("âŒ ë¬¸ì„œ ì²­í‚¹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
    raise ValueError("ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨ - ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤")

print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(chunks)}")



# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
vector_store = MilvusVectorStore(
    collection_name=COLLECTION_NAME, 
    embedding_model=embedding_model,
    metric_type='IP',
    index_type='HNSW',
    milvus_host=MILVUS_SERVER_IP,  
    milvus_port=MILVUS_PORT   
)
    
# ë¬¸ì„œë¥¼ vector DBì— ì¶”ê°€
print(f"\nğŸ“¤ {len(chunks)}ê°œ ë¬¸ì„œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤...")
inserted_ids = vector_store.add_documents(chunks)
print(f"âœ… ì‚½ì…ëœ ë¬¸ì„œ ìˆ˜: {len(inserted_ids)}")


# top_k íƒ€ì… ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
print(f"\nğŸ”§ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
retriever = get_retriever(vector_store, retriever_type='top_k')


# RAG ì²´ì¸ êµ¬ì„±
rag_chain = (
    RunnableParallel(
        context=retriever,
        question=RunnablePassthrough()
    )
    | RAG_prompt
    | llm
    | StrOutputParser()
)


async def process_with_rag(question: str) -> str:
    print("ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œì‘")
    response = rag_chain.invoke(question)
    print(f"âœ… RAG ì‘ë‹µ ìƒì„±: {len(response)} ë¬¸ì")
    return response


# RAG ì²´ì¸ìœ¼ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
async def stream_rag_response(question: str, model_name: str):
    try:
        print(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question}")

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¨¼ì € ì²´í¬ (ë” ì¼ë°˜ì )
        if hasattr(rag_chain, 'stream'):
            for chunk in rag_chain.stream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²´í¬
        elif hasattr(rag_chain, 'astream'):  
            async for chunk in rag_chain.astream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)  # ì‘ì€ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
        
        # ë°©ë²• 2: ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì‹œ ì²­í¬ë¡œ ë¶„í•  (í˜„ì¬ ë°©ì‹ ê°œì„ )
        else:
            response_content = await asyncio.get_event_loop().run_in_executor(
                None, rag_chain.invoke, question
            )
            print(f"ğŸ” ì „ì²´ ì‘ë‹µ: {response_content}")
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
            words = response_content.split()
            chunk_size = 1  # 3ë‹¨ì–´ì”©
            
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i+chunk_size]
                chunk = " ".join(chunk_words)
                if i + chunk_size < len(words):
                    chunk += " "  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
                
                response_chunk = {
                    "id": f"chatcmpl-{uuid.uuid4()}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model_name,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": chunk},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(response_chunk)}\n\n"
                await asyncio.sleep(0.03) 
        
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
# RAG ì²´ì¸ìœ¼ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
async def stream_rag_response(question: str, model_name: str):
    try:
        print(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question}")

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¨¼ì € ì²´í¬ (ë” ì¼ë°˜ì )
        if hasattr(rag_chain, 'stream'):
            for chunk in rag_chain.stream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²´í¬
        elif hasattr(rag_chain, 'astream'):  
            async for chunk in rag_chain.astream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)  # ì‘ì€ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
        
        # ë°©ë²• 2: ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì‹œ ì²­í¬ë¡œ ë¶„í•  (í˜„ì¬ ë°©ì‹ ê°œì„ )
        else:
            response_content = await asyncio.get_event_loop().run_in_executor(
                None, rag_chain.invoke, question
            )
            print(f"ğŸ” ì „ì²´ ì‘ë‹µ: {response_content}")
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
            words = response_content.split()
            chunk_size = 1  # 3ë‹¨ì–´ì”©
            
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i+chunk_size]
                chunk = " ".join(chunk_words)
                if i + chunk_size < len(words):
                    chunk += " "  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
                
                response_chunk = {
                    "id": f"chatcmpl-{uuid.uuid4()}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model_name,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": chunk},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(response_chunk)}\n\n"
                await asyncio.sleep(0.03) 
        
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
        final_chunk = {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"
        yield "data: [DONE]\n\n"  # OpenAI ìŠ¤íƒ€ì¼ ì¢…ë£Œ ì‹ í˜¸
        
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        # Open WebUI í˜¸í™˜ ì—ëŸ¬ í˜•ì‹
        error_response = {
            "error": {
                "message": str(e),
                "type": "internal_server_error",
                "code": "rag_error"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
        yield "data: [DONE]\n\n"
# RAG ì²´ì¸ìœ¼ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
async def stream_rag_response(question: str, model_name: str):
    try:
        print(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question}")

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¨¼ì € ì²´í¬ (ë” ì¼ë°˜ì )
        if hasattr(rag_chain, 'stream'):
            for chunk in rag_chain.stream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²´í¬
        elif hasattr(rag_chain, 'astream'):  
            async for chunk in rag_chain.astream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)  # ì‘ì€ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
        
        # ë°©ë²• 2: ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì‹œ ì²­í¬ë¡œ ë¶„í•  (í˜„ì¬ ë°©ì‹ ê°œì„ )
        else:
            response_content = await asyncio.get_event_loop().run_in_executor(
                None, rag_chain.invoke, question
            )
            print(f"ğŸ” ì „ì²´ ì‘ë‹µ: {response_content}")
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
            words = response_content.split()
            chunk_size = 1  # 3ë‹¨ì–´ì”©
            
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i+chunk_size]
                chunk = " ".join(chunk_words)
                if i + chunk_size < len(words):
                    chunk += " "  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
                
                response_chunk = {
                    "id": f"chatcmpl-{uuid.uuid4()}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model_name,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": chunk},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(response_chunk)}\n\n"
                await asyncio.sleep(0.03) 
        
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
        final_chunk = {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        # Open WebUI í˜¸í™˜ ì—ëŸ¬ í˜•ì‹
        error_response = {
            "error": {
                "message": str(e),
                "type": "internal_server_error",
                "code": "rag_error"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
        yield "data: [DONE]\n\n"

        
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        # Open WebUI í˜¸í™˜ ì—ëŸ¬ í˜•ì‹
        error_response = {
            "error": {
                "message": str(e),
                "type": "internal_server_error",
                "code": "rag_error"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
        yield "data: [DONE]\n\n"



# FastAPI ì•± ìƒì„±
app = FastAPI(title="RAG Server", description="RAG API ì„œë²„", version="1.0.0")
print(f"\nâœ… FastAPI ì•± ìƒì„± \n")


##############################
### Open Web UI ì—”ë“œí¬ì¸íŠ¸ ###
##############################
'''
/api/chat/completions : ê°€ì¥ ì¤‘ìš”í•œ ì—”ë“œí¬ì¸íŠ¸, Open WebUIë¡œ ë¶€í„° ì „ë‹¬ë°›ì€ ì§ˆë¬¸ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ëë‚´ê³  íšŒì‹ 
/api/models : í˜„ì¬ Open WebUIë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì˜ ëª©ë¡ì„ ì „ë‹¬
/api/tags : ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì „ë‹¬


'''

@app.post("/api/chat/completions", response_model=ChatResponse)
async def chat_completions(
    request: ChatRequest,
    authorization: Optional[str] = Header(None)
    ):
    try:
        if authorization:
            print(f"ğŸ”‘ ì¸ì¦: {authorization[:20]}...")
        
        print(f"\nğŸ¯ POST : /api/chat/completions")
        print(f"   ëª¨ë¸: {request.model}")
        print(f"   ìŠ¤íŠ¸ë¦¼: {request.stream}")
        
        user_question = request.messages[-1].content
        print(f"   ì§ˆë¬¸: {user_question}")
        
        if request.stream:
            return StreamingResponse(
                stream_rag_response(user_question, request.model),
                media_type="text/plain",
                headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                }
            )
        else:
            response_content = rag_chain.invoke(user_question)
            print(f"âœ… ì‘ë‹µ: {response_content}")
            
            return ChatResponse(
                id=f"chatcmpl-{uuid.uuid4()}",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatResponseChoice(
                        index=0,
                        message=ChatResponseMessage(role="assistant", content=response_content)
                    )
                ]
            )
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models", response_model=ModelsResponse)
async def list_models():
    return ModelsResponse(
        object="list",  
        data=[
            ModelInfo(
                id="rag-cheeseade",
                object="model",  
                owned_by="CHEESEADE",
                created=int(time.time()),  
                permission=[],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                root="rag-cheeseade",  # ë³´í†µ idì™€ ë™ì¼
            )
        ]
    )















# @app.middleware("http")
# async def api_key_middleware(request: Request, call_next):
#     """API í‚¤ ê²€ì¦ì„ ê±´ë„ˆë›°ëŠ” ë¯¸ë“¤ì›¨ì–´"""
#     # Authorization í—¤ë”ê°€ ìˆì–´ë„ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
#     response = await call_next(request)
#     return response

# # Pydantic ëª¨ë¸ë“¤
# class Message(BaseModel):
#     role: str
#     content: str

# class ChatRequest(BaseModel):
#     model: str
#     messages: List[Message]
#     stream: bool = False
#     max_tokens: Optional[int] = None
#     temperature: Optional[float] = None

# class StreamChoiceDelta(BaseModel):
#     content: str

# class StreamChoice(BaseModel):
#     index: int = 0
#     delta: StreamChoiceDelta
#     finish_reason: Optional[str] = None

# class StreamResponse(BaseModel):
#     id: str
#     object: str = "chat.completion.chunk"
#     created: int
#     model: str
#     choices: List[StreamChoice]

# class ChatResponseMessage(BaseModel):
#     role: str
#     content: str

# class ChatResponseChoice(BaseModel):
#     index: int
#     message: ChatResponseMessage
#     finish_reason: Optional[str] = "stop"

# class ChatResponse(BaseModel):
#     id: str
#     object: str = "chat.completion"
#     created: int
#     model: str
#     choices: List[ChatResponseChoice]
#     usage: Optional[dict] = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

# class ModelInfo(BaseModel):
#     id: str
#     object: str = "model"
#     created: int = 1677652288
#     owned_by: str = "rag-server"

# class ModelsResponse(BaseModel):
#     object: str = "list"
#     data: List[ModelInfo]






# # Open WebUIê°€ ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì—”ë“œí¬ì¸íŠ¸
# @app.post("/api/chat/completions")
# async def alt_chat_completions(request: ChatRequest):
#     """ëŒ€ì•ˆ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
#     return await openai_chat_completions(request)



# @app.post("/api/chat")
# async def chat_with_rag(request: ChatRequest):
#     """ëª¨ë¸ ì„ íƒì— ë”°ë¥¸ RAG/ì§ì ‘ LLM ì²˜ë¦¬"""
#     try:
#         print(f"\nğŸ¯ ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ :")
#         print(f"   ëª¨ë¸: {request.model}")
#         print(f"   ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}")
#         print(f"   ìŠ¤íŠ¸ë¦¼: {request.stream}")
        
#         user_question = request.messages[-1].content
#         print(f"   ì§ˆë¬¸: {user_question[:100]}...")
        
#         # Open WebUI ë‚´ë¶€ ì‹œìŠ¤í…œ ìš”ì²­ í•„í„°ë§
#         if any(keyword in user_question.lower() for keyword in [
#             "### task:", "follow-up", "follow_ups", "generate", "suggest", "tags"
#         ]):
#             print("ğŸš« ì‹œìŠ¤í…œ ìš”ì²­ ê°ì§€ - ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
#             return ChatResponse(
#                 id=f"chatcmpl-{uuid.uuid4()}",
#                 created=int(time.time()),
#                 model=request.model,
#                 choices=[
#                     ChatResponseChoice(
#                         index=0,
#                         message=ChatResponseMessage(
#                             role="assistant", 
#                             content="ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”."
#                         )
#                     )
#                 ]
#             )
        
#         # ëª¨ë¸ë³„ ì²˜ë¦¬ ë¶„ê¸°
#         if is_rag_model(request.model):
#             print("ğŸ” RAG ëª¨ë¸ ì„ íƒë¨ - RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬")
#             response_content = await process_with_rag(user_question)
            
#         elif is_direct_model(request.model):
#             print("ğŸ¯ ì§ì ‘ LLM ëª¨ë¸ ì„ íƒë¨ - ë°”ë¡œ LLM ì²˜ë¦¬")
#             response_content = await process_direct_llm(user_question, request.model)
            
#         else:
#             print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {request.model} - ê¸°ë³¸ RAG ì²˜ë¦¬")
#             response_content = await process_with_rag(user_question)
        
#         # ì‘ë‹µ ê²€ì¦ - ë¹ˆ ì‘ë‹µ ë°©ì§€
#         if not response_content or len(response_content.strip()) < 3:
#             response_content = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
#             print("âš ï¸ ë¹ˆ ì‘ë‹µ ê°ì§€ - ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´")
        
#         print(f"âœ… ìµœì¢… ì‘ë‹µ: {len(response_content)} ë¬¸ì")
#         print(f"ğŸ” ì‘ë‹µ ë‚´ìš©: '{response_content[:200]}...'")
        
#         # ì¤‘ìš”: ìŠ¤íŠ¸ë¦¼ ìš”ì²­ì´ì–´ë„ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬ (ë””ë²„ê¹…ìš©)
#         print("ğŸ“¤ ì¼ë°˜ ì‘ë‹µ ëª¨ë“œë¡œ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¼ ìš°íšŒ)")
        
#         response = ChatResponse(
#             id=f"chatcmpl-{uuid.uuid4()}",
#             created=int(time.time()),
#             model=request.model,
#             choices=[
#                 ChatResponseChoice(
#                     index=0, 
#                     message=ChatResponseMessage(
#                         role="assistant", 
#                         content=response_content  # ì—¬ê¸°ê°€ í•µì‹¬!
#                     )
#                 )
#             ],
#             usage={
#                 "prompt_tokens": len(user_question.split()),
#                 "completion_tokens": len(response_content.split()),
#                 "total_tokens": len(user_question.split()) + len(response_content.split())
#             }
#         )
        
#         print(f"ğŸ“¤ ì‘ë‹µ ê°ì²´ ìƒì„± ì™„ë£Œ: content='{response.choices[0].message.content[:100]}...'")
#         return response
            
#     except Exception as e:
#         print(f"âŒ Error in chat endpoint: {str(e)}")
#         import traceback
#         traceback.print_exc()
        
#         # ì—ëŸ¬ ì‹œì—ë„ ìœ íš¨í•œ ì‘ë‹µ ë°˜í™˜
#         return ChatResponse(
#             id=f"chatcmpl-{uuid.uuid4()}",
#             created=int(time.time()),
#             model=request.model,
#             choices=[
#                 ChatResponseChoice(
#                     index=0,
#                     message=ChatResponseMessage(
#                         role="assistant",
#                         content="ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
#                     )
#                 )
#             ]
#         )

# # ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
# async def stream_rag_response_simple(response_content: str, model_name: str):
#     """ê°œì„ ëœ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ - Open WebUI í˜¸í™˜"""
#     try:
#         print(f"ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: '{response_content[:50]}...'")
        
#         # ë¬¸ì ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ë” ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼)
#         chunk_size = 5  # 5ë¬¸ìì”©
#         for i in range(0, len(response_content), chunk_size):
#             chunk = response_content[i:i+chunk_size]
            
#             response_chunk = {
#                 "id": f"chatcmpl-{uuid.uuid4()}",
#                 "object": "chat.completion.chunk",
#                 "created": int(time.time()),
#                 "model": model_name,
#                 "choices": [{
#                     "index": 0,
#                     "delta": {"content": chunk},
#                     "finish_reason": None
#                 }]
#             }
            
#             # í•œê¸€ ê¹¨ì§ ë°©ì§€
#             chunk_json = json.dumps(response_chunk, ensure_ascii=False)
#             yield f"data: {chunk_json}\n\n"
            
#             # ì•½ê°„ì˜ ì§€ì—°ìœ¼ë¡œ íƒ€ì´í•‘ íš¨ê³¼
#             import asyncio
#             await asyncio.sleep(0.05)
        
#         # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
#         final_chunk = {
#             "id": f"chatcmpl-{uuid.uuid4()}",
#             "object": "chat.completion.chunk", 
#             "created": int(time.time()),
#             "model": model_name,
#             "choices": [{
#                 "index": 0,
#                 "delta": {},
#                 "finish_reason": "stop"
#             }]
#         }
        
#         final_json = json.dumps(final_chunk, ensure_ascii=False)
#         yield f"data: {final_json}\n\n"
#         yield "data: [DONE]\n\n"
        
#         print(f"ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        
#     except Exception as e:
#         print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
#         error_response = {
#             "id": f"chatcmpl-{uuid.uuid4()}",
#             "object": "chat.completion.chunk",
#             "created": int(time.time()),
#             "model": model_name,
#             "choices": [{
#                 "index": 0,
#                 "delta": {"content": f"ì˜¤ë¥˜: {str(e)}"},
#                 "finish_reason": "stop"
#             }]
#         }
#         yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
#         yield "data: [DONE]\n\n"

# # Open WebUIìš© ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤
# @app.post("/api/generate")
# async def generate(request: dict):
#     """Ollama generate API í˜¸í™˜"""
#     try:
#         prompt = request.get("prompt", "")
#         model = request.get("model", "rag-samsung")
#         stream = request.get("stream", False)
        
#         print(f"ğŸ¯ Generate ìš”ì²­: model={model}, stream={stream}")
#         print(f"   í”„ë¡¬í”„íŠ¸: {prompt}")
        
#         if stream:
#             # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
#             async def generate_stream():
#                 async for chunk in rag_chain.astream(prompt):
#                     if chunk:
#                         response = {
#                             "model": model,
#                             "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
#                             "response": chunk,
#                             "done": False
#                         }
#                         yield f"{json.dumps(response)}\n"
                
#                 # ì™„ë£Œ ì‹ í˜¸
#                 final_response = {
#                     "model": model,
#                     "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
#                     "response": "",
#                     "done": True
#                 }
#                 yield f"{json.dumps(final_response)}\n"
            
#             return StreamingResponse(generate_stream(), media_type="application/json")
#         else:
#             # ì¼ë°˜ ì‘ë‹µ
#             response_content = rag_chain.invoke(prompt)
#             return {
#                 "model": model,
#                 "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
#                 "response": response_content,
#                 "done": True
#             }
#     except Exception as e:
#         print(f"âŒ Generate ì˜¤ë¥˜: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.post("/api/pull")
# async def pull_model(request: dict):
#     """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œë¡œëŠ” ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)"""
#     return {
#         "status": "success",
#         "digest": "sha256:abcd1234",
#         "total": 1000000000,
#         "completed": 1000000000
#     }

# @app.delete("/api/delete")
# async def delete_model(request: dict):
#     """ëª¨ë¸ ì‚­ì œ (ì‹¤ì œë¡œëŠ” ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)"""
#     return {"status": "success"}

# @app.post("/api/copy")
# async def copy_model(request: dict):
#     """ëª¨ë¸ ë³µì‚¬ (ì‹¤ì œë¡œëŠ” ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)"""
#     return {"status": "success"}

# @app.post("/api/create")
# async def create_model(request: dict):
#     """ëª¨ë¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)"""
#     return {"status": "success"}

# @app.post("/api/push")
# async def push_model(request: dict):
#     """ëª¨ë¸ ì—…ë¡œë“œ (ì‹¤ì œë¡œëŠ” ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)"""
#     return {"status": "success"}

# @app.post("/api/show")
# async def show_model(request: dict):
#     """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
#     return {
#         "license": "Apache 2.0",
#         "modelfile": "FROM rag-samsung:latest",
#         "parameters": "temperature 0.7",
#         "template": "{{ .Prompt }}",
#         "details": {
#             "parent_model": "",
#             "format": "gguf",
#             "family": "gemma3",
#             "families": ["gemma3"],
#             "parameter_size": "27B",
#             "quantization_level": "Q4_K_M"
#         }
#     }

# @app.get("/health")
# async def health_check():
#     return {"status": True}

# @app.get("/")
# async def root():
#     """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - Open WebUI í˜¸í™˜"""
#     return {
#         "message": "Samsung RAG Server",
#         "version": "1.0.0",
#         "status": "running"
#     }

# @app.get("/api")
# async def api_root():
#     """API ë£¨íŠ¸"""
#     return {
#         "message": "Samsung RAG API",
#         "version": "1.0.0",
#         "endpoints": ["/api/tags", "/api/chat", "/api/generate"]
#     }

# @app.get("/debug/test-openwebui")
# async def test_open_webui_format():
#     """Open WebUI í˜•ì‹ í…ŒìŠ¤íŠ¸"""
#     return {
#         "id": "chatcmpl-test123",
#         "object": "chat.completion",
#         "created": int(time.time()),
#         "model": "rag-samsung",
#         "choices": [
#             {
#                 "index": 0,
#                 "message": {
#                     "role": "assistant",
#                     "content": "í…ŒìŠ¤íŠ¸ ì‘ë‹µì…ë‹ˆë‹¤. ì´ê²ƒì´ ë³´ì´ë©´ í˜•ì‹ì´ ì˜¬ë°”ë¦…ë‹ˆë‹¤."
#                 },
#                 "finish_reason": "stop"
#             }
#         ],
#         "usage": {
#             "prompt_tokens": 10,
#             "completion_tokens": 20,
#             "total_tokens": 30
#         }
#     }

# @app.post("/debug/direct-chat")
# async def direct_chat(query: dict):
#     """ì§ì ‘ RAG í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
#     try:
#         question = query.get("question", "ì•ˆë…•")
#         print(f"\nğŸ§ª ì§ì ‘ í…ŒìŠ¤íŠ¸: {question}")
        
#         response = rag_chain.invoke(question)
#         print(f"âœ… ì§ì ‘ ì‘ë‹µ: {response}")
        
#         return {
#             "question": question,
#             "response": response,
#             "length": len(response)
#         }
#     except Exception as e:
#         print(f"âŒ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
#         return {"error": str(e)}



# @app.options("/api/chat")
# async def chat_options():
#     """CORS preflight ìš”ì²­ ì²˜ë¦¬"""
#     return {"message": "OK"}

# @app.get("/api/version")
# async def api_version():
#     """API ë²„ì „ ì •ë³´ (Open WebUI í•„ìˆ˜)"""
#     return {
#         "version": "0.1.0",
#         "build": "samsung-rag-2025"
#     }

# @app.get("/version")
# async def get_version_alt():
#     """ëŒ€ì²´ ë²„ì „ ì—”ë“œí¬ì¸íŠ¸"""
#     return {"version": "0.1.0"}

# # Open WebUIê°€ ìš”ì²­í•˜ëŠ” ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤
# # Open WebUIê°€ ìš”ì²­í•˜ëŠ” ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤
# @app.get("/ollama/api/tags")
# async def ollama_get_models():
#     """Open WebUIìš© Ollama íƒœê·¸ ì—”ë“œí¬ì¸íŠ¸"""
#     return {
#         "models": [
#             {
#                 "name": "rag-samsung:latest",
#                 "model": "rag-samsung:latest", 
#                 "modified_at": "2024-01-01T00:00:00Z",
#                 "size": 1000000000,
#                 "digest": "sha256:abcd1234",
#                 "details": {
#                     "parent_model": "",
#                     "format": "gguf", 
#                     "family": "gemma3",
#                     "families": ["gemma3"],
#                     "parameter_size": "27B",
#                     "quantization_level": "Q4_K_M"
#                 }
#             }
#         ]
#     }

# @app.get("/ollama/api/version")
# async def ollama_get_version():
#     """Open WebUIìš© Ollama ë²„ì „ ì—”ë“œí¬ì¸íŠ¸"""
#     return {"version": "0.1.0"}

# @app.post("/ollama/api/chat")
# async def ollama_chat(request: ChatRequest):
#     """Open WebUIìš© Ollama ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
#     return await chat_with_rag(request)

# @app.post("/ollama/api/generate")
# async def ollama_generate(request: dict):
#     """Open WebUIìš© Ollama ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
#     return await generate(request)

# @app.get("/api/ps")
# async def get_running_models():
#     """ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡"""
#     return {
#         "models": [
#             {
#                 "name": "rag-samsung:latest", 
#                 "model": "rag-samsung:latest",
#                 "size": 1000000000,
#                 "digest": "sha256:abcd1234",
#                 "details": {
#                     "parent_model": "",
#                     "format": "gguf",
#                     "family": "gemma3",
#                     "families": ["gemma3"],
#                     "parameter_size": "27B",
#                     "quantization_level": "Q4_K_M"
#                 },
#                 "expires_at": "2024-12-31T23:59:59Z",
#                 "size_vram": 500000000
#             }
#         ]
#     }

# @app.post("/debug/test-retrieval")
# async def test_retrieval(query: dict):
#     """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
#     try:
#         question = query.get("question", "ì¹´ë©”ë¼")
#         docs = retriever.invoke(question)
#         return {
#             "question": question,
#             "retrieved_docs": len(docs),
#             "docs": [
#                 {
#                     "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
#                     "metadata": doc.metadata
#                 }
#                 for doc in docs
#             ]
#         }
#     except Exception as e:
#         return {"error": str(e)}

# # server.pyì— ì¶”ê°€í•  ì—”ë“œí¬ì¸íŠ¸ë“¤ (API í‚¤ ê´€ë ¨)

# @app.get("/api/v1/auths/api_key")
# async def get_api_key():
#     """API í‚¤ ìƒíƒœ í™•ì¸ (Open WebUI í˜¸í™˜)"""
#     return {"api_key": None, "status": "disabled"}

# @app.post("/api/v1/auths/api_key")
# async def set_api_key(request: dict):
#     """API í‚¤ ì„¤ì • (ë”ë¯¸ ì—”ë“œí¬ì¸íŠ¸)"""
#     return {"status": "success", "message": "API key not required"}

# @app.delete("/api/v1/auths/api_key")
# async def delete_api_key():
#     """API í‚¤ ì‚­ì œ (ë”ë¯¸ ì—”ë“œí¬ì¸íŠ¸)"""
#     return {"status": "success"}

# # Open WebUI ì¸ì¦ ê´€ë ¨ ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸
# @app.get("/api/auth")
# async def auth_status():
#     """ì¸ì¦ ìƒíƒœ í™•ì¸"""
#     return {"authenticated": True, "user": "anonymous"}

# @app.get("/api/config")
# async def get_config():
#     """Open WebUI ì„¤ì • ì •ë³´"""
#     return {
#         "name": "Samsung AI Assistant",
#         "version": "1.0.0",
#         "auth": False,
#         "default_models": ["rag-samsung:latest", "gemma3:27b-it-q4_K_M"],
#         "features": {
#             "enable_signup": True,
#             "enable_login": False,
#             "enable_api_key": False
#         }
#     }

# # OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS preflight)
# @app.options("/{full_path:path}")
# async def options_handler(full_path: str):
#     """ëª¨ë“  ê²½ë¡œì— ëŒ€í•œ OPTIONS ìš”ì²­ ì²˜ë¦¬"""
#     return {"message": "OK"}

# # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì¶”ê°€
# from fastapi import Request
# from fastapi.responses import JSONResponse

# @app.exception_handler(404)
# async def not_found_handler(request: Request, exc):
#     """404 ì—ëŸ¬ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜"""
#     return JSONResponse(
#         status_code=404,
#         content={
#             "error": "Not Found",
#             "message": f"Endpoint {request.url.path} not found",
#             "available_endpoints": [
#                 "/health", "/api/tags", "/api/chat", "/api/version"
#             ]
#         }
#     )

# @app.exception_handler(500)
# async def internal_error_handler(request: Request, exc):
#     """500 ì—ëŸ¬ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜"""
#     return JSONResponse(
#         status_code=500,
#         content={
#             "error": "Internal Server Error",
#             "message": str(exc) if hasattr(exc, 'detail') else "Unknown error"
#         }
#     )

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
#     allow_credentials=True,
#     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
#     allow_headers=["*"],
# )

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")







# # -------------------------------------------------------------------------------
# # -------------------------------------------------------------------------------
# # -------------------------------------------------------------------------------
# # -------------------------------------------------------------------------------


# # RAG ì „ìš© FastAPI ì„œë²„
# import os
# from fastapi import FastAPI
# from langchain_ollama import ChatOllama

# # í™˜ê²½ë³€ìˆ˜
# try:
#     LLM_SERVER_URL = os.environ["LLM_SERVER_URL"]
# except KeyError:
#     raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ì¸ 'LLM_SERVER_URL'ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” .env.global íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
# try:
#     RAG_MODEL_NAME = os.environ["RAG_MODEL_NAME"]
# except KeyError:
#     raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ì¸ 'RAG_MODEL_NAME'ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” .env.global íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


# app = FastAPI(title="CHEESEADE RAG Server", version="1.0.0")

# @app.get("/api/tags")
# async def get_models():
#     """RAG ëª¨ë¸ë§Œ ì œê³µ"""
#     return {
#         "models": [{
#             "name": RAG_MODEL_NAME,
#             "model": RAG_MODEL_NAME,
#             "details": {
#                 "description": "CHEESEADE RAGë¥¼ í™œìš©í•œ ì „ë¬¸ ìƒë‹´"
#             }
#         }]
#     }

# @app.post("/api/chat")
# async def chat_with_rag(request: ChatRequest):
#     """RAG ì²˜ë¦¬ ì „ìš©"""
#     # í•­ìƒ RAG íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
#     response = rag_chain.invoke(request.messages[-1].content)
#     return create_chat_response(response, RAG_MODEL_NAME)

# @app.get("/health")
# async def health_check():
#     return {"status": "healthy", "service": "rag-server"}


# @app.get("/api/tags")
# async def get_models():
#     try:
#         return {
#             "models": [
#                 {
#                     "name": "rag-samsung:latest",
#                     "model": "rag-samsung:latest",
#                     "modified_at": "2024-01-01T00:00:00Z",
#                     "size": 1000000000,
#                     "digest": "sha256:rag001",
#                     "details": {
#                         "family": "rag-enhanced",
#                         "parameter_size": "RAG + 27B",
#                         "quantization_level": "Q4_K_M",
#                         "description": "Samsung ì œí’ˆ ì „ìš© RAG ì–´ì‹œìŠ¤í„´íŠ¸"
#                     }
#                 },
#                 {
#                     "name": "gemma3:27b-it-q4_K_M",
#                     "model": "gemma3:27b-it-q4_K_M", 
#                     "modified_at": "2024-01-01T00:00:00Z",
#                     "size": 15000000000,
#                     "digest": "sha256:gem001",
#                     "details": {
#                         "family": "gemma3",
#                         "parameter_size": "27B",
#                         "quantization_level": "Q4_K_M",
#                         "description": "ì¼ë°˜ìš© ëŒ€í™”í˜• AI ëª¨ë¸"
#                     }
#                 }
#             ]
#         }
#     except Exception as e:
#         print(f"Error in /api/tags: {e}")
#         return {"models": []}