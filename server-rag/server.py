import os
import uvicorn
import uuid
import time
import json
import requests
import torch
import asyncio

from typing import List, Optional
from fastapi import FastAPI, HTTPException, Request, Header, status, Response
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.runnables import RunnableParallel

from chunking.chunking_md import chunk_markdown_file
from embedding.bge_m3 import get_bge_m3_model
from retriever.retriever import get_retriever
from vector_db.milvus import MilvusVectorStore
from api.routes import router as openwebui_router

# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
LLM_SERVER_URL = os.environ["LLM_SERVER_URL"]
RAG_MODEL_NAME = os.environ["RAG_MODEL_NAME"]
MILVUS_SERVER_IP = os.environ["MILVUS_SERVER_IP"]
MILVUS_PORT = os.environ["MILVUS_PORT"]
LLM_MODEL_NAME = os.environ["LLM_MODEL_NAME"]
COLLECTION_NAME = os.environ["COLLECTION_NAME"]

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
system_prompt = '''Answer the user's Question from the Context.
Keep your answer ground in the facts of the Context.
If the Context doesn't contain the facts to answer, just output 'ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
Please answer in Korean.'''

# LLM ì„œë²„ì— ì—°ê²°
try:
    print(f"ğŸ”— LLM ì„œë²„ ì—°ê²° ì‹œë„: {LLM_SERVER_URL}")
    response = requests.get(f"{LLM_SERVER_URL}/api/tags", timeout=10)
    if response.status_code == 200:
        print(f"âœ… LLM ì„œë²„ ì—°ê²° ì„±ê³µ: {LLM_SERVER_URL}")
    else:
        print(f"âš ï¸ LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        
    llm = ChatOllama(
        model=LLM_MODEL_NAME,
        base_url=LLM_SERVER_URL,
        timeout=120
    )
    print(f"âœ… LLM ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    
except requests.exceptions.ConnectionError:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {LLM_SERVER_URL}")
    llm = None
except Exception as e:
    print(f"âŒ LLM ì„œë²„ ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
    llm = None
 
# RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
RAG_prompt = ChatPromptTemplate([
    ('system', system_prompt),
    ('user', '''Context: {context}
    ---
     Question: {question}''')
])

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU ìš°ì„  ì‚¬ìš©)
if torch.cuda.is_available():
    print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    device = 'cuda'
else:
    print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš©")
    device = 'cpu'

print(f"ğŸ”§ {device}ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„")
embedding_model = get_bge_m3_model()
print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# 3. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
markdown_path = "./docs/feature.md"

print(f"\nğŸ“ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘...")
chunks = chunk_markdown_file(markdown_path)

if len(chunks) == 0:
    print("âŒ ë¬¸ì„œ ì²­í‚¹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
    raise ValueError("ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨ - ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤")

print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(chunks)}")

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
vector_store = MilvusVectorStore(
    collection_name=COLLECTION_NAME, 
    embedding_model=embedding_model,
    metric_type='IP',
    index_type='HNSW',
    milvus_host=MILVUS_SERVER_IP,  
    milvus_port=MILVUS_PORT   
)
    
# ë¬¸ì„œë¥¼ vector DBì— ì¶”ê°€
print(f"\nğŸ“¤ {len(chunks)}ê°œ ë¬¸ì„œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤...")
inserted_ids = vector_store.add_documents(chunks)
print(f"âœ… ì‚½ì…ëœ ë¬¸ì„œ ìˆ˜: {len(inserted_ids)}")

# top_k íƒ€ì… ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
print(f"\nğŸ”§ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
retriever = get_retriever(vector_store, retriever_type='top_k')

# RAG ì²´ì¸ êµ¬ì„±
rag_chain = (
    RunnableParallel(
        context=retriever, 
        question=RunnablePassthrough()
    )
    | RAG_prompt
    | llm
    | StrOutputParser()
)

async def process_with_rag(question: str) -> str:
    print("ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œì‘")
    response = rag_chain.invoke(question)
    print(f"âœ… RAG ì‘ë‹µ ìƒì„±: {len(response)} ë¬¸ì")
    return response

# RAG ì²´ì¸ìœ¼ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
async def stream_rag_response(question: str, model_name: str):
    try:
        print(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question}")

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¨¼ì € ì²´í¬ (ë” ì¼ë°˜ì )
        if hasattr(rag_chain, 'stream'):
            for chunk in rag_chain.stream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²´í¬
        elif hasattr(rag_chain, 'astream'):  
            async for chunk in rag_chain.astream(question):
                if chunk:
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03)
        
        # ë°©ë²• 3: ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì‹œ ì²­í¬ë¡œ ë¶„í• 
        else:
            response_content = await asyncio.get_event_loop().run_in_executor(
                None, rag_chain.invoke, question
            )
            print(f"ğŸ” ì „ì²´ ì‘ë‹µ: {response_content}")
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
            words = response_content.split()
            chunk_size = 1  # 1ë‹¨ì–´ì”©
            
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i+chunk_size]
                chunk = " ".join(chunk_words)
                if i + chunk_size < len(words):
                    chunk += " "  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
                
                response_chunk = {
                    "id": f"chatcmpl-{uuid.uuid4()}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model_name,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": chunk},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(response_chunk)}\n\n"
                await asyncio.sleep(0.03) 
        
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
        final_chunk = {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        # Open WebUI í˜¸í™˜ ì—ëŸ¬ í˜•ì‹
        error_response = {
            "error": {
                "message": str(e),
                "type": "internal_server_error",
                "code": "rag_error"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
        yield "data: [DONE]\n\n"



# FastAPI ì•± ìƒì„±
app = FastAPI(title="RAG Server", description="RAG API ì„œë²„", version="1.0.0")
print(f"\nâœ… FastAPI ì•± ìƒì„± \n")

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# server.pyì˜ ê¸°ì¡´ FastAPI ì•±ì— ì¶”ê°€í•  ë¶€ë¶„

# ================================
# OpenWebUI API ë¼ìš°í„° ì¶”ê°€
# ================================

# ë¼ìš°í„° ë“±ë¡ (FastAPI ì•± ìƒì„± í›„ ì¶”ê°€)
app.include_router(openwebui_router)

print(f"âœ… OpenWebUI API ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
print(f"     GET /api/models - ëª¨ë¸ ëª©ë¡ (ì¸ì¦ í•„ìš”)")
print(f"     GET /api/tags - Ollama í˜¸í™˜ íƒœê·¸ (ì¸ì¦ í•„ìš”)")
print(f"     GET /api/show?name=<model> - ëª¨ë¸ ìƒì„¸ (ì¸ì¦ í•„ìš”)")
print(f"     GET /api/version - API ë²„ì „ (ì¸ì¦ ë¶ˆí•„ìš”)")
print(f"     GET /api/health - ì„œë²„ ìƒíƒœ (ì¸ì¦ ë¶ˆí•„ìš”)")

# ================================
# ì¸ì¦ ê´€ë ¨ ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒì‚¬í•­)
# ================================

# @app.post("/api/auth/signin")
# async def signin(credentials: dict):
#     """ê°„ë‹¨í•œ ë¡œê·¸ì¸ API (ê°œë°œìš©)"""
#     # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì DB í™•ì¸
#     if credentials.get("email") == "dev@cheeseade.com" and credentials.get("password") == "dev123":
#         return {
#             "token": "sk-cheeseade-dev-key-001",
#             "token_type": "Bearer",
#             "user": {
#                 "id": "dev-user",
#                 "email": "dev@cheeseade.com",
#                 "name": "Developer"
#             }
#         }
    
#     raise HTTPException(status_code=401, detail="Invalid credentials")

# @app.get("/api/auth/me")
# async def get_me(current_user: dict = Depends(get_current_user)):
#     """í˜„ì¬ ì‚¬ìš©ì ì •ë³´"""
#     return {
#         "user": current_user,
#         "authenticated": True
#     }















##############################
### Open Web UI ì—”ë“œí¬ì¸íŠ¸ ###
##############################

# 1. GET /api/models

# ì‘ë‹µ ëª¨ë¸ ì •ì˜ (OpenWebUI í˜•ì‹ì— ë§ì¶¤)
class Model(BaseModel):
    id: str
    name: str
    # í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: "object": "model", "owned_by": "user", "permissions": []

class ModelList(BaseModel):
    models: List[Model]

curl -H "Authorization: Bearer YOUR_API_KEY" http://localhost:3000/api/models

@app.get("/api/models", response_model=ModelList)
async def list_models():
    return ModelsResponse(
        object="list",  
        data=[
            ModelInfo(
                id=RAG_MODEL_NAME,
                object="model",  
                owned_by="CHEESEADE",
                created=int(time.time()),  
                permission=[],
                root=RAG_MODEL_NAME,
            )
        ]
    )



@app.post("/api/chat", response_model=ChatResponse)
async def chat_completions(
    request: ChatMessage,
    authorization: Optional[str] = Header(None)
):
    try:
        if authorization:
            print(f"ğŸ”‘ ì¸ì¦: {authorization[:20]}...")
        
        print(f"\nğŸ¯ POST : /api/chat/completions")
        print(f"   ëª¨ë¸: {request.model}")
        print(f"   ìŠ¤íŠ¸ë¦¼: {request.stream}")
        
        user_question = request.messages[-1].content
        print(f"   ì§ˆë¬¸: {user_question}")
        
        if request.stream:
            return StreamingResponse(
                stream_rag_response(user_question, request.model),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        else:
            response_content = rag_chain.invoke(user_question)
            print(f"âœ… ì‘ë‹µ: {response_content}")
            
            return ChatResponse(
                id=f"chatcmpl-{uuid.uuid4()}",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatResponseChoice(
                        index=0,
                        message=ChatResponseMessage(role="assistant", content=response_content)
                    )
                ]
            )
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@app.get("/api/tags")
async def get_models():
    """RAG ëª¨ë¸ë§Œ ì œê³µ"""
    return {
        "models": [{
            "name": RAG_MODEL_NAME,
            "model": RAG_MODEL_NAME,
            "modified_at": "2024-01-01T00:00:00Z",
            "size": 1000000000,
            "digest": "sha256:rag001",
            "details": {
                "family": "rag-enhanced",
                "parameter_size": "RAG + 27B",
                "quantization_level": "Q4_K_M",
                "description": "CHEESEADE RAGë¥¼ í™œìš©í•œ ì „ë¬¸ ìƒë‹´"
            }
        }]
    }

@app.get("/health")
def health_check(response: Response):
    print(f'ğŸ“‹ /health ì‘ë‹µ')
    response.status_code = status.HTTP_200_OK
    return {"status": "healthy", "service": "rag-server"}

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - Open WebUI í˜¸í™˜"""
    return {
        "message": "CHEESEADE RAG Server",
        "version": "1.0.0",
        "status": "running"
    }

@app.post("/debug/test-retrieval")
async def test_retrieval(query: dict):
    """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        question = query.get("question", "ì¹´ë©”ë¼")
        docs = retriever.invoke(question)
        return {
            "question": question,
            "retrieved_docs": len(docs),
            "docs": [
                {
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")















# # Pydantic ëª¨ë¸ë“¤
# class Message(BaseModel):
#     role: str
#     content: str

# class ChatRequest(BaseModel):
#     model: str
#     messages: List[Message]
#     stream: bool = False
#     max_tokens: Optional[int] = None
#     temperature: Optional[float] = None

# class ChatResponseMessage(BaseModel):
#     role: str
#     content: str

# class ChatResponseChoice(BaseModel):
#     index: int
#     message: ChatResponseMessage
#     finish_reason: Optional[str] = "stop"

# class ChatResponse(BaseModel):
#     id: str
#     object: str = "chat.completion"
#     created: int
#     model: str
#     choices: List[ChatResponseChoice]
#     usage: Optional[dict] = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}



# class ModelsResponse(BaseModel):
#     object: str = "list"
#     data: List[ModelInfo]

# class ChatMessage(BaseModel):
#     message: str
#     # í•„ìš”ì— ë”°ë¼ user_id, session_id ë“± ì¶”ê°€ í•„ë“œë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#     # user_id: str | None = None