"""
OpenWebUI ì±„íŒ… ì²˜ë¦¬ ëª¨ë“ˆ
"""
import uuid
import time
import json
import asyncio
import requests
from typing import Optional, AsyncGenerator
from fastapi import HTTPException

from .models import ChatRequest, ChatResponse, ChatResponseChoice, ChatResponseMessage

class ChatHandler:
    """ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, rag_chain, retriever, rag_model_name: str, llm_server_url: str):
        self.rag_chain = rag_chain
        self.retriever = retriever
        self.rag_model_name = rag_model_name
        self.llm_server_url = llm_server_url
        
        print(f"ğŸ’¬ ChatHandler ì´ˆê¸°í™”")
        print(f"   RAG ëª¨ë¸: {rag_model_name}")
        print(f"   LLM ì„œë²„: {llm_server_url}")
    
    async def process_with_rag(self, question: str) -> str:
        """RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬"""
        print(f"ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬: {question}")
        
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            response = await asyncio.get_event_loop().run_in_executor(
                None, self.rag_chain.invoke, question
            )
            print(f"âœ… RAG ì‘ë‹µ ìƒì„±: {len(response)} ë¬¸ì")
            return response
        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"RAG ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    async def stream_rag_response(self, question: str, model_name: str) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° RAG ì‘ë‹µ ìƒì„±"""
        try:
            print(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question}")

            # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¨¼ì € ì²´í¬ (ë” ì¼ë°˜ì )
            if hasattr(self.rag_chain, 'stream'):
                for chunk in self.rag_chain.stream(question):
                    if chunk:
                        response_chunk = {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model_name,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": chunk},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(response_chunk)}\n\n"
                        await asyncio.sleep(0.03)

            # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²´í¬
            elif hasattr(self.rag_chain, 'astream'):  
                async for chunk in self.rag_chain.astream(question):
                    if chunk:
                        response_chunk = {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model_name,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": chunk},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(response_chunk)}\n\n"
                        await asyncio.sleep(0.03)
            
            # ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì‹œ ì²­í¬ë¡œ ë¶„í• 
            else:
                response_content = await asyncio.get_event_loop().run_in_executor(
                    None, self.rag_chain.invoke, question
                )
                print(f"ğŸ” ì „ì²´ ì‘ë‹µ: {response_content}")
                
                # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
                words = response_content.split()
                chunk_size = 1  # 1ë‹¨ì–´ì”©
                
                for i in range(0, len(words), chunk_size):
                    chunk_words = words[i:i+chunk_size]
                    chunk = " ".join(chunk_words)
                    if i + chunk_size < len(words):
                        chunk += " "  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
                    
                    response_chunk = {
                        "id": f"chatcmpl-{uuid.uuid4()}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_name,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
                    await asyncio.sleep(0.03) 
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            final_chunk = {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion.chunk", 
                "created": int(time.time()),
                "model": model_name,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(final_chunk)}\n\n"
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            # OpenWebUI í˜¸í™˜ ì—ëŸ¬ í˜•ì‹
            error_response = {
                "error": {
                    "message": str(e),
                    "type": "internal_server_error",
                    "code": "rag_error"
                }
            }
            yield f"data: {json.dumps(error_response)}\n\n"
            yield "data: [DONE]\n\n"
    
    async def proxy_to_llm(self, request: ChatRequest) -> dict:
        """ì¼ë°˜ LLMìœ¼ë¡œ í”„ë¡ì‹œ (ë…¼ìŠ¤íŠ¸ë¦¬ë°)"""
        print(f"ğŸ”„ ì¼ë°˜ LLMìœ¼ë¡œ í”„ë¡ì‹œ: {request.model}")
        
        # Ollama APIë¡œ ìš”ì²­ ì „ë‹¬
        ollama_request = {
            "model": request.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "stream": False
        }
        
        try:
            proxy_response = requests.post(
                f"{self.llm_server_url}/v1/chat/completions",
                json=ollama_request,
                timeout=120
            )
            
            if proxy_response.status_code == 200:
                return proxy_response.json()
            else:
                raise HTTPException(
                    status_code=proxy_response.status_code,
                    detail=f"LLM server error: {proxy_response.text}"
                )
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ í”„ë¡ì‹œ ìš”ì²­ ì˜¤ë¥˜: {e}")
            raise HTTPException(
                status_code=502,
                detail=f"Failed to connect to LLM server: {str(e)}"
            )
    
    async def proxy_stream_to_llm(self, request: ChatRequest) -> AsyncGenerator[bytes, None]:
        """ì¼ë°˜ LLMìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° í”„ë¡ì‹œ"""
        print(f"ğŸŒŠ ì¼ë°˜ LLM ìŠ¤íŠ¸ë¦¬ë° í”„ë¡ì‹œ: {request.model}")
        
        # Ollama APIë¡œ ìš”ì²­ ì „ë‹¬
        ollama_request = {
            "model": request.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "stream": True
        }
        
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.llm_server_url}/v1/chat/completions",
                    json=ollama_request,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    async for chunk in response.content.iter_chunked(1024):
                        yield chunk
        except Exception as e:
            print(f"âŒ í”„ë¡ì‹œ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
            error_chunk = {
                "error": {
                    "message": f"Proxy error: {str(e)}",
                    "type": "proxy_error"
                }
            }
            yield f"data: {json.dumps(error_chunk)}\n\n".encode()
    
    async def handle_chat_request(self, request: ChatRequest) -> dict:
        """ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ (ë©”ì¸ í•¸ë“¤ëŸ¬)"""
        user_question = request.messages[-1].content
        print(f"ğŸ’¬ ì±„íŒ… ìš”ì²­ ì²˜ë¦¬: {request.model}")
        print(f"   ì§ˆë¬¸: {user_question}")
        
        # ëª¨ë¸ì´ RAG ëª¨ë¸ì¸ì§€ í™•ì¸
        if request.model == self.rag_model_name:
            # RAG ì‚¬ìš©
            response_content = await self.process_with_rag(user_question)
            
            return ChatResponse(
                id=f"chatcmpl-{uuid.uuid4()}",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatResponseChoice(
                        index=0,
                        message=ChatResponseMessage(role="assistant", content=response_content)
                    )
                ]
            ).dict()
        else:
            # ì¼ë°˜ LLM ì‚¬ìš© (í”„ë¡ì‹œ)
            return await self.proxy_to_llm(request)
    
    def test_retrieval(self, question: str) -> dict:
        """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ§ª ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: {question}")
        
        try:
            docs = self.retriever.invoke(question)
            
            response_docs = []
            for doc in docs:
                doc_data = {
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "metadata": doc.metadata,
                    "score": doc.metadata.get("score", 0.0)
                }
                response_docs.append(doc_data)
            
            print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
            
            return {
                "question": question,
                "retrieved_docs": len(docs),
                "docs": response_docs
            }
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
            )