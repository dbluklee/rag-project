"""
Ollama API
ê³µì‹ ë¬¸ì„œ : https://github.com/ollama/ollama/blob/main/docs/api.md

"""
import json
import time
import asyncio
import requests
import traceback
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# ë¼ìš°í„° ìƒì„±
router = APIRouter()

# ì „ì—­ ì±„íŒ… í•¸ë“¤ëŸ¬ (server.pyì—ì„œ ì„¤ì •)
chat_handler = None

def set_chat_handler(handler):
    """ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì •"""
    global chat_handler
    chat_handler = handler
    print(f"âœ… [SET_HANDLER] ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")
    print(f"   [SET_HANDLER] RAG ëª¨ë¸ëª…: {getattr(handler, 'rag_model_name', 'UNKNOWN')}")
    print(f"   [SET_HANDLER] LLM ì„œë²„ URL: {getattr(handler, 'llm_server_url', 'UNKNOWN')}")

# ================================
# Ollama API ëª¨ë¸ ì •ì˜ (ìµœì†Œí•œ)
# ================================

class OllamaMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str
    images: Optional[List[str]] = None

class OllamaChatRequest(BaseModel):
    model: str
    messages: List[OllamaMessage]
    stream: Optional[bool] = False
    options: Optional[Dict[str, Any]] = None

class OllamaGenerateRequest(BaseModel):
    model: str
    prompt: str
    stream: Optional[bool] = False
    options: Optional[Dict[str, Any]] = None
    system: Optional[str] = None

# ================================
# í•µì‹¬ API ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.post("/api/chat")
async def chat_ollama(request: OllamaChatRequest):
    """
    Ollama ì±„íŒ… API
    POST /api/chat
    """
    print(f"\nğŸ¯ [CHAT_START] POST /api/chat ì‹œì‘")
    print(f"   [CHAT_START] ëª¨ë¸: {request.model}")
    print(f"   [CHAT_START] ìŠ¤íŠ¸ë¦¼: {request.stream}")
    print(f"   [CHAT_START] ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}")
    
    # ì±„íŒ… í•¸ë“¤ëŸ¬ í™•ì¸
    if not chat_handler:
        print(f"âŒ [CHAT_ERROR] ì±„íŒ… í•¸ë“¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        raise HTTPException(status_code=503, detail="Chat handler not initialized")
    
    print(f"âœ… [CHAT_CHECK] ì±„íŒ… í•¸ë“¤ëŸ¬ í™•ì¸ë¨")
    
    # ë©”ì‹œì§€ ì¶œë ¥
    for i, msg in enumerate(request.messages):
        print(f"   [CHAT_MSG_{i}] {msg.role}: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}")
    
    # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
    print(f"ğŸ” [CHAT_PARSE] ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ëŠ” ì¤‘...")
    user_message = next((msg for msg in reversed(request.messages) if msg.role == "user"), None)
    
    if not user_message:
        print(f"âŒ [CHAT_ERROR] ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        raise HTTPException(status_code=400, detail="No user message found")
    
    question = user_message.content
    print(f"âœ… [CHAT_PARSE] ì‚¬ìš©ì ì§ˆë¬¸: {question}")
    
    try:
        print(f"ğŸ” [CHAT_MODEL_CHECK] ëª¨ë¸ í™•ì¸ ì¤‘...")
        print(f"   [CHAT_MODEL_CHECK] ìš”ì²­ ëª¨ë¸: {request.model}")
        print(f"   [CHAT_MODEL_CHECK] RAG ëª¨ë¸: {chat_handler.rag_model_name}")
        
        # RAG ëª¨ë¸ì¸ì§€ í™•ì¸
        if request.model == chat_handler.rag_model_name:
            print(f"âœ… [CHAT_RAG] RAG ëª¨ë¸ë¡œ ì²˜ë¦¬ ì‹œì‘")
            
            if request.stream:
                print(f"ğŸŒŠ [CHAT_RAG_STREAM] ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì²˜ë¦¬")
                return StreamingResponse(
                    rag_chat_stream(question, request.model),
                    media_type="application/x-ndjson"
                )
            else:
                print(f"ğŸ“ [CHAT_RAG_SYNC] ë…¼ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì²˜ë¦¬")
                print(f"   [CHAT_RAG_SYNC] RAG ì²˜ë¦¬ ì‹œì‘...")
                response_content = await chat_handler.process_with_rag(question)
                print(f"   [CHAT_RAG_SYNC] RAG ì‘ë‹µ ê¸¸ì´: {len(response_content)} ë¬¸ì")
                print(f"   [CHAT_RAG_SYNC] RAG ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_content[:200]}...")
                
                response = create_chat_response(request.model, response_content)
                print(f"âœ… [CHAT_RAG_SYNC] ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                return response
        else:
            print(f"ğŸ”„ [CHAT_PROXY] ì¼ë°˜ LLMìœ¼ë¡œ í”„ë¡ì‹œ ì²˜ë¦¬")
            print(f"   [CHAT_PROXY] LLM ì„œë²„ URL: {chat_handler.llm_server_url}")
            
            result = await proxy_chat_to_ollama(request)
            print(f"âœ… [CHAT_PROXY] í”„ë¡ì‹œ ì™„ë£Œ")
            return result
            
    except Exception as e:
        print(f"âŒ [CHAT_EXCEPTION] ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        print(f"   [CHAT_EXCEPTION] ì—ëŸ¬: {str(e)}")
        print(f"   [CHAT_EXCEPTION] ìƒì„¸: {traceback.format_exc()}")
        
        error_response = create_chat_error_response(request.model, str(e))
        print(f"   [CHAT_EXCEPTION] ì—ëŸ¬ ì‘ë‹µ ìƒì„±ë¨")
        return error_response

@router.post("/api/generate")
async def generate_ollama(request: OllamaGenerateRequest):
    """
    Ollama ìƒì„± API
    POST /api/generate
    """
    print(f"\nğŸ¯ [GENERATE_START] POST /api/generate ì‹œì‘")
    print(f"   [GENERATE_START] ëª¨ë¸: {request.model}")
    print(f"   [GENERATE_START] ìŠ¤íŠ¸ë¦¼: {request.stream}")
    print(f"   [GENERATE_START] í”„ë¡¬í”„íŠ¸: {request.prompt[:100]}{'...' if len(request.prompt) > 100 else ''}")
    
    # ì±„íŒ… í•¸ë“¤ëŸ¬ í™•ì¸
    if not chat_handler:
        print(f"âŒ [GENERATE_ERROR] ì±„íŒ… í•¸ë“¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        raise HTTPException(status_code=503, detail="Chat handler not initialized")
    
    print(f"âœ… [GENERATE_CHECK] ì±„íŒ… í•¸ë“¤ëŸ¬ í™•ì¸ë¨")
    
    try:
        print(f"ğŸ” [GENERATE_MODEL_CHECK] ëª¨ë¸ í™•ì¸ ì¤‘...")
        print(f"   [GENERATE_MODEL_CHECK] ìš”ì²­ ëª¨ë¸: {request.model}")
        print(f"   [GENERATE_MODEL_CHECK] RAG ëª¨ë¸: {chat_handler.rag_model_name}")
        
        # RAG ëª¨ë¸ì¸ì§€ í™•ì¸
        if request.model == chat_handler.rag_model_name:
            print(f"âœ… [GENERATE_RAG] RAG ëª¨ë¸ë¡œ ì²˜ë¦¬ ì‹œì‘")
            
            if request.stream:
                print(f"ğŸŒŠ [GENERATE_RAG_STREAM] ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì²˜ë¦¬")
                return StreamingResponse(
                    rag_generate_stream(request.prompt, request.model),
                    media_type="application/x-ndjson"
                )
            else:
                print(f"ğŸ“ [GENERATE_RAG_SYNC] ë…¼ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì²˜ë¦¬")
                print(f"   [GENERATE_RAG_SYNC] RAG ì²˜ë¦¬ ì‹œì‘...")
                response_content = await chat_handler.process_with_rag(request.prompt)
                print(f"   [GENERATE_RAG_SYNC] RAG ì‘ë‹µ ê¸¸ì´: {len(response_content)} ë¬¸ì")
                print(f"   [GENERATE_RAG_SYNC] RAG ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_content[:200]}...")
                
                response = create_generate_response(request.model, response_content)
                print(f"âœ… [GENERATE_RAG_SYNC] ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                return response
        else:
            print(f"ğŸ”„ [GENERATE_PROXY] ì¼ë°˜ LLMìœ¼ë¡œ í”„ë¡ì‹œ ì²˜ë¦¬")
            print(f"   [GENERATE_PROXY] LLM ì„œë²„ URL: {chat_handler.llm_server_url}")
            
            result = await proxy_generate_to_ollama(request)
            print(f"âœ… [GENERATE_PROXY] í”„ë¡ì‹œ ì™„ë£Œ")
            return result
            
    except Exception as e:
        print(f"âŒ [GENERATE_EXCEPTION] ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        print(f"   [GENERATE_EXCEPTION] ì—ëŸ¬: {str(e)}")
        print(f"   [GENERATE_EXCEPTION] ìƒì„¸: {traceback.format_exc()}")
        
        error_response = create_generate_error_response(request.model, str(e))
        print(f"   [GENERATE_EXCEPTION] ì—ëŸ¬ ì‘ë‹µ ìƒì„±ë¨")
        return error_response

# ================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# ================================

def create_chat_response(model: str, content: str, done: bool = True):
    """Ollama ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    print(f"ğŸ”§ [CREATE_CHAT_RESP] ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘...")
    print(f"   [CREATE_CHAT_RESP] ëª¨ë¸: {model}")
    print(f"   [CREATE_CHAT_RESP] ë‚´ìš© ê¸¸ì´: {len(content)} ë¬¸ì")
    print(f"   [CREATE_CHAT_RESP] ì™„ë£Œ ìƒíƒœ: {done}")
    
    response = {
        "model": model,
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
        "message": {
            "role": "assistant",
            "content": content
        },
        "done": done,
        "total_duration": 1000000000,
        "load_duration": 100000000,
        "prompt_eval_count": 10,
        "prompt_eval_duration": 200000000,
        "eval_count": len(content.split()),
        "eval_duration": 500000000
    }
    
    print(f"âœ… [CREATE_CHAT_RESP] ì±„íŒ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")
    return response

def create_generate_response(model: str, content: str, done: bool = True):
    """Ollama ìƒì„± ì‘ë‹µ ìƒì„±"""
    print(f"ğŸ”§ [CREATE_GEN_RESP] ìƒì„± ì‘ë‹µ ìƒì„± ì¤‘...")
    print(f"   [CREATE_GEN_RESP] ëª¨ë¸: {model}")
    print(f"   [CREATE_GEN_RESP] ë‚´ìš© ê¸¸ì´: {len(content)} ë¬¸ì")
    print(f"   [CREATE_GEN_RESP] ì™„ë£Œ ìƒíƒœ: {done}")
    
    response = {
        "model": model,
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
        "response": content,
        "done": done,
        "context": [],
        "total_duration": 1000000000,
        "load_duration": 100000000,
        "prompt_eval_count": 10,
        "prompt_eval_duration": 200000000,
        "eval_count": len(content.split()),
        "eval_duration": 500000000
    }
    
    print(f"âœ… [CREATE_GEN_RESP] ìƒì„± ì‘ë‹µ ìƒì„± ì™„ë£Œ")
    return response

def create_chat_error_response(model: str, error: str):
    """Ollama ì±„íŒ… ì—ëŸ¬ ì‘ë‹µ"""
    print(f"âŒ [CREATE_CHAT_ERROR] ì±„íŒ… ì—ëŸ¬ ì‘ë‹µ ìƒì„±")
    print(f"   [CREATE_CHAT_ERROR] ëª¨ë¸: {model}")
    print(f"   [CREATE_CHAT_ERROR] ì—ëŸ¬: {error}")
    
    return {
        "model": model,
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
        "message": {
            "role": "assistant",
            "content": f"Error: {error}"
        },
        "done": True
    }

def create_generate_error_response(model: str, error: str):
    """Ollama ìƒì„± ì—ëŸ¬ ì‘ë‹µ"""
    print(f"âŒ [CREATE_GEN_ERROR] ìƒì„± ì—ëŸ¬ ì‘ë‹µ ìƒì„±")
    print(f"   [CREATE_GEN_ERROR] ëª¨ë¸: {model}")
    print(f"   [CREATE_GEN_ERROR] ì—ëŸ¬: {error}")
    
    return {
        "model": model,
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
        "response": f"Error: {error}",
        "done": True
    }

# ================================
# ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ë“¤
# ================================

async def rag_chat_stream(question: str, model: str):
    """RAG ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (Ollama í˜•ì‹)"""
    print(f"ğŸŒŠ [RAG_CHAT_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
    print(f"   [RAG_CHAT_STREAM] ì§ˆë¬¸: {question}")
    print(f"   [RAG_CHAT_STREAM] ëª¨ë¸: {model}")
    
    try:
        print(f"   [RAG_CHAT_STREAM] RAG ì²˜ë¦¬ ì‹œì‘...")
        # RAG ì‘ë‹µ ìƒì„±
        response_content = await chat_handler.process_with_rag(question)
        print(f"   [RAG_CHAT_STREAM] RAG ì‘ë‹µ ì™„ë£Œ: {len(response_content)} ë¬¸ì")
        print(f"   [RAG_CHAT_STREAM] ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_content[:100]}...")
        
        # ë‹¨ì–´ë³„ë¡œ ë¶„í• í•´ì„œ ìŠ¤íŠ¸ë¦¬ë°
        words = response_content.split()
        chunk_size = 2  # 2ë‹¨ì–´ì”©
        total_chunks = (len(words) + chunk_size - 1) // chunk_size
        
        print(f"   [RAG_CHAT_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜: {total_chunks}")
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i+chunk_size]
            chunk = " ".join(chunk_words)
            if i + chunk_size < len(words):
                chunk += " "
            
            chunk_num = i // chunk_size + 1
            print(f"   [RAG_CHAT_STREAM] ì²­í¬ {chunk_num}/{total_chunks}: '{chunk}'")
            
            # Ollama ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹
            chunk_response = {
                "model": model,
                "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
                "message": {
                    "role": "assistant",
                    "content": chunk
                },
                "done": False
            }
            
            yield json.dumps(chunk_response) + "\n"
            await asyncio.sleep(0.03)
        
        print(f"   [RAG_CHAT_STREAM] ëª¨ë“  ì²­í¬ ì „ì†¡ ì™„ë£Œ")
        
        # ì¢…ë£Œ ì‘ë‹µ
        final_response = {
            "model": model,
            "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
            "message": {
                "role": "assistant",
                "content": ""
            },
            "done": True,
            "total_duration": 1000000000,
            "load_duration": 100000000,
            "prompt_eval_count": len(question.split()),
            "prompt_eval_duration": 200000000,
            "eval_count": len(response_content.split()),
            "eval_duration": 500000000
        }
        
        print(f"   [RAG_CHAT_STREAM] ì¢…ë£Œ ì‘ë‹µ ì „ì†¡")
        yield json.dumps(final_response) + "\n"
        print(f"âœ… [RAG_CHAT_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ [RAG_CHAT_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        print(f"   [RAG_CHAT_STREAM] ìƒì„¸: {traceback.format_exc()}")
        
        error_response = create_chat_error_response(model, str(e))
        yield json.dumps(error_response) + "\n"

async def rag_generate_stream(prompt: str, model: str):
    """RAG ìƒì„± ìŠ¤íŠ¸ë¦¬ë° (Ollama í˜•ì‹)"""
    print(f"ğŸŒŠ [RAG_GEN_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
    print(f"   [RAG_GEN_STREAM] í”„ë¡¬í”„íŠ¸: {prompt}")
    print(f"   [RAG_GEN_STREAM] ëª¨ë¸: {model}")
    
    try:
        print(f"   [RAG_GEN_STREAM] RAG ì²˜ë¦¬ ì‹œì‘...")
        # RAG ì‘ë‹µ ìƒì„±
        response_content = await chat_handler.process_with_rag(prompt)
        print(f"   [RAG_GEN_STREAM] RAG ì‘ë‹µ ì™„ë£Œ: {len(response_content)} ë¬¸ì")
        print(f"   [RAG_GEN_STREAM] ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_content[:100]}...")
        
        # ë‹¨ì–´ë³„ë¡œ ë¶„í• í•´ì„œ ìŠ¤íŠ¸ë¦¬ë°
        words = response_content.split()
        chunk_size = 2
        total_chunks = (len(words) + chunk_size - 1) // chunk_size
        
        print(f"   [RAG_GEN_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜: {total_chunks}")
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i+chunk_size]
            chunk = " ".join(chunk_words)
            if i + chunk_size < len(words):
                chunk += " "
            
            chunk_num = i // chunk_size + 1
            print(f"   [RAG_GEN_STREAM] ì²­í¬ {chunk_num}/{total_chunks}: '{chunk}'")
            
            # Ollama ìƒì„± ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹
            chunk_response = {
                "model": model,
                "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
                "response": chunk,
                "done": False
            }
            
            yield json.dumps(chunk_response) + "\n"
            await asyncio.sleep(0.03)
        
        print(f"   [RAG_GEN_STREAM] ëª¨ë“  ì²­í¬ ì „ì†¡ ì™„ë£Œ")
        
        # ì¢…ë£Œ ì‘ë‹µ
        final_response = {
            "model": model,
            "created_at": time.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
            "response": "",
            "done": True,
            "context": [],
            "total_duration": 1000000000,
            "load_duration": 100000000,
            "prompt_eval_count": len(prompt.split()),
            "prompt_eval_duration": 200000000,
            "eval_count": len(response_content.split()),
            "eval_duration": 500000000
        }
        
        print(f"   [RAG_GEN_STREAM] ì¢…ë£Œ ì‘ë‹µ ì „ì†¡")
        yield json.dumps(final_response) + "\n"
        print(f"âœ… [RAG_GEN_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ [RAG_GEN_STREAM] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        print(f"   [RAG_GEN_STREAM] ìƒì„¸: {traceback.format_exc()}")
        
        error_response = create_generate_error_response(model, str(e))
        yield json.dumps(error_response) + "\n"

# ================================
# í”„ë¡ì‹œ í•¨ìˆ˜ë“¤
# ================================

async def proxy_chat_to_ollama(request: OllamaChatRequest):
    """ì±„íŒ…ì„ LLM ì„œë²„ë¡œ í”„ë¡ì‹œ"""
    print(f"ğŸ”„ [PROXY_CHAT] LLM ì„œë²„ë¡œ í”„ë¡ì‹œ ì‹œì‘")
    print(f"   [PROXY_CHAT] ëŒ€ìƒ URL: {chat_handler.llm_server_url}/api/chat")
    print(f"   [PROXY_CHAT] ìš”ì²­ ëª¨ë¸: {request.model}")
    
    try:
        request_data = request.dict()
        print(f"   [PROXY_CHAT] ìš”ì²­ ë°ì´í„° í¬ê¸°: {len(json.dumps(request_data))} bytes")
        
        print(f"   [PROXY_CHAT] HTTP ìš”ì²­ ì „ì†¡ ì¤‘...")
        response = requests.post(
            f"{chat_handler.llm_server_url}/api/chat",
            json=request_data,
            timeout=120
        )
        
        print(f"   [PROXY_CHAT] HTTP ì‘ë‹µ ìˆ˜ì‹ : {response.status_code}")
        print(f"   [PROXY_CHAT] ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
        
        if response.status_code == 200:
            response_data = response.json()
            print(f"   [PROXY_CHAT] JSON íŒŒì‹± ì„±ê³µ")
            print(f"   [PROXY_CHAT] ì‘ë‹µ í•„ë“œ: {list(response_data.keys())}")
            print(f"âœ… [PROXY_CHAT] í”„ë¡ì‹œ ì„±ê³µ")
            return response_data
        else:
            print(f"âŒ [PROXY_CHAT] HTTP ì—ëŸ¬: {response.status_code}")
            print(f"   [PROXY_CHAT] ì—ëŸ¬ ë‚´ìš©: {response.text[:200]}")
            return create_chat_error_response(request.model, f"LLM server error: {response.status_code}")
            
    except requests.exceptions.Timeout:
        print(f"âŒ [PROXY_CHAT] íƒ€ì„ì•„ì›ƒ ì—ëŸ¬")
        return create_chat_error_response(request.model, "Request timeout")
    except requests.exceptions.ConnectionError:
        print(f"âŒ [PROXY_CHAT] ì—°ê²° ì—ëŸ¬")
        return create_chat_error_response(request.model, "Connection error")
    except Exception as e:
        print(f"âŒ [PROXY_CHAT] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        print(f"   [PROXY_CHAT] ìƒì„¸: {traceback.format_exc()}")
        return create_chat_error_response(request.model, f"Proxy error: {str(e)}")

async def proxy_generate_to_ollama(request: OllamaGenerateRequest):
    """ìƒì„±ì„ LLM ì„œë²„ë¡œ í”„ë¡ì‹œ"""
    print(f"ğŸ”„ [PROXY_GEN] LLM ì„œë²„ë¡œ í”„ë¡ì‹œ ì‹œì‘")
    print(f"   [PROXY_GEN] ëŒ€ìƒ URL: {chat_handler.llm_server_url}/api/generate")
    print(f"   [PROXY_GEN] ìš”ì²­ ëª¨ë¸: {request.model}")
    
    try:
        request_data = request.dict()
        print(f"   [PROXY_GEN] ìš”ì²­ ë°ì´í„° í¬ê¸°: {len(json.dumps(request_data))} bytes")
        
        print(f"   [PROXY_GEN] HTTP ìš”ì²­ ì „ì†¡ ì¤‘...")
        response = requests.post(
            f"{chat_handler.llm_server_url}/api/generate",
            json=request_data,
            timeout=120
        )
        
        print(f"   [PROXY_GEN] HTTP ì‘ë‹µ ìˆ˜ì‹ : {response.status_code}")
        print(f"   [PROXY_GEN] ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
        
        if response.status_code == 200:
            response_data = response.json()
            print(f"   [PROXY_GEN] JSON íŒŒì‹± ì„±ê³µ")
            print(f"   [PROXY_GEN] ì‘ë‹µ í•„ë“œ: {list(response_data.keys())}")
            print(f"âœ… [PROXY_GEN] í”„ë¡ì‹œ ì„±ê³µ")
            return response_data
        else:
            print(f"âŒ [PROXY_GEN] HTTP ì—ëŸ¬: {response.status_code}")
            print(f"   [PROXY_GEN] ì—ëŸ¬ ë‚´ìš©: {response.text[:200]}")
            return create_generate_error_response(request.model, f"LLM server error: {response.status_code}")
            
    except requests.exceptions.Timeout:
        print(f"âŒ [PROXY_GEN] íƒ€ì„ì•„ì›ƒ ì—ëŸ¬")
        return create_generate_error_response(request.model, "Request timeout")
    except requests.exceptions.ConnectionError:
        print(f"âŒ [PROXY_GEN] ì—°ê²° ì—ëŸ¬")
        return create_generate_error_response(request.model, "Connection error")
    except Exception as e:
        print(f"âŒ [PROXY_GEN] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        print(f"   [PROXY_GEN] ìƒì„¸: {traceback.format_exc()}")
        return create_generate_error_response(request.model, f"Proxy error: {str(e)}")

@router.get("/api/ps")
async def list_running_models():
    """
    ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ API (Ollama í˜•ì‹)
    GET /api/ps
    """
    print(f"ğŸ”„ [PS] ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ìš”ì²­")
    
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        import os
        rag_model_name = os.environ.get("RAG_MODEL_NAME", "rag-cheeseade:latest")
        llm_model_name = os.environ.get("LLM_MODEL_NAME", "gemma3:27b-it-q4_K_M")
        
        print(f"   [PS] RAG ëª¨ë¸: {rag_model_name}")
        print(f"   [PS] LLM ëª¨ë¸: {llm_model_name}")
        
        # ì±„íŒ… í•¸ë“¤ëŸ¬ ìƒíƒœ í™•ì¸
        handler_loaded = chat_handler is not None
        print(f"   [PS] ì±„íŒ… í•¸ë“¤ëŸ¬ ë¡œë“œë¨: {handler_loaded}")
        
        # í˜„ì¬ ì‹œê°„ ê³„ì‚°
        current_time = time.time()
        load_time = current_time - 3600  # 1ì‹œê°„ ì „ì— ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •
        
        models = []
        
        # RAG ëª¨ë¸ì´ ë¡œë“œëœ ê²ƒìœ¼ë¡œ í‘œì‹œ (í•­ìƒ ë¡œë“œëœ ìƒíƒœ)
        if handler_loaded:
            models.append({
                "name": rag_model_name,
                "model": rag_model_name,
                "size": 2500000000,
                "digest": f"sha256:{abs(hash(rag_model_name)):064x}",
                "details": {
                    "parent_model": "",
                    "format": "gguf",
                    "family": "rag-enhanced",
                    "families": ["rag-enhanced"],
                    "parameter_size": "RAG+27B",
                    "quantization_level": "Q4_K_M"
                },
                "expires_at": "2024-12-01T23:59:59.999999999Z",  # ë§Œë£Œ ì‹œê°„
                "size_vram": 2147483648  # VRAM ì‚¬ìš©ëŸ‰ (2GB)
            })
            print(f"   [PS] RAG ëª¨ë¸ í™œì„± ìƒíƒœë¡œ ì¶”ê°€")
        
        # LLM ì„œë²„ ìƒíƒœ í™•ì¸
        try:
            print(f"   [PS] LLM ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘...")
            llm_ps_response = requests.get(
                f"{chat_handler.llm_server_url}/api/ps",
                timeout=5
            )
            print(f"   [PS] LLM ì„œë²„ ì‘ë‹µ: {llm_ps_response.status_code}")
            
            if llm_ps_response.status_code == 200:
                llm_data = llm_ps_response.json()
                print(f"   [PS] LLM ì„œë²„ ì‹¤í–‰ ëª¨ë¸: {len(llm_data.get('models', []))}ê°œ")
                
                # LLM ì„œë²„ì˜ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤ ì¶”ê°€
                for llm_model in llm_data.get('models', []):
                    models.append(llm_model)
                    print(f"   [PS] LLM ëª¨ë¸ ì¶”ê°€: {llm_model.get('name', 'unknown')}")
            else:
                print(f"   [PS] LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {llm_ps_response.status_code}")
                
        except requests.exceptions.RequestException as e:
            print(f"   [PS] LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            # LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ëª¨ë¸ì€ í‘œì‹œ
        except Exception as e:
            print(f"   [PS] LLM ì„œë²„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # Ollama /api/ps í‘œì¤€ ì‘ë‹µ í˜•ì‹
        response = {
            "models": models
        }
        
        print(f"âœ… [PS] ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ì‘ë‹µ: {len(models)}ê°œ ëª¨ë¸")
        for i, model in enumerate(models):
            model_name = model.get('name', 'unknown')
            model_size = model.get('size_vram', model.get('size', 0))
            print(f"   [PS] ëª¨ë¸ {i+1}: {model_name} (VRAM: {model_size//1024//1024}MB)")
        
        return response
        
    except Exception as e:
        print(f"âŒ [PS] ì‹¤í–‰ ëª¨ë¸ ëª©ë¡ ì˜¤ë¥˜: {str(e)}")
        print(f"   [PS] ìƒì„¸: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ì‹œì—ë„ Ollama í˜•ì‹ ìœ ì§€
        return {
            "models": []
        }

# ================================
# í—¬ìŠ¤ì²´í¬ ë° ëª¨ë¸ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/health")
async def health_check():
    """
    í—¬ìŠ¤ì²´í¬ API
    GET /health
    """
    print(f"ğŸ¥ [HEALTH] í—¬ìŠ¤ì²´í¬ ìš”ì²­")
    
    try:
        # ê¸°ë³¸ ìƒíƒœ í™•ì¸
        print(f"   [HEALTH] ì±„íŒ… í•¸ë“¤ëŸ¬ ìƒíƒœ í™•ì¸...")
        handler_status = "initialized" if chat_handler else "not_initialized"
        print(f"   [HEALTH] í•¸ë“¤ëŸ¬ ìƒíƒœ: {handler_status}")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        rag_model = os.environ.get("RAG_MODEL_NAME", "unknown")
        llm_model = os.environ.get("LLM_MODEL_NAME", "unknown")
        print(f"   [HEALTH] RAG ëª¨ë¸: {rag_model}")
        print(f"   [HEALTH] LLM ëª¨ë¸: {llm_model}")
        
        health_data = {
            "status": "healthy" if chat_handler else "degraded",
            "service": "cheeseade-rag-server",
            "timestamp": int(time.time()),
            "chat_handler": handler_status,
            "models": {
                "rag_model": rag_model,
                "llm_model": llm_model
            }
        }
        
        print(f"âœ… [HEALTH] í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return health_data
        
    except Exception as e:
        print(f"âŒ [HEALTH] í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        print(f"   [HEALTH] ìƒì„¸: {traceback.format_exc()}")
        
        return {
            "status": "unhealthy",
            "service": "cheeseade-rag-server", 
            "timestamp": int(time.time()),
            "error": str(e)
        }

@router.get("/api/tags")
async def list_local_models():
    """
    ë¡œì»¬ ëª¨ë¸ ëª©ë¡ API (Ollama í˜•ì‹)
    GET /api/tags
    """
    print(f"ğŸ“‹ [TAGS] Ollama ëª¨ë¸ ëª©ë¡ ìš”ì²­")
    
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        import os
        rag_model_name = os.environ.get("RAG_MODEL_NAME", "rag-cheeseade:latest")
        llm_model_name = os.environ.get("LLM_MODEL_NAME", "gemma3:27b-it-q4_K_M")
        
        print(f"   [TAGS] RAG ëª¨ë¸: {rag_model_name}")
        print(f"   [TAGS] LLM ëª¨ë¸: {llm_model_name}")
        
        # Ollama ì •í™•í•œ /api/tags ì‘ë‹µ í˜•ì‹
        models = [
            {
                "name": rag_model_name,
                "model": rag_model_name,
                "modified_at": "2024-12-01T00:00:00.000000000Z",
                "size": 2500000000,
                "digest": f"sha256:{abs(hash(rag_model_name)):064x}",
                "details": {
                    "parent_model": "",
                    "format": "gguf",
                    "family": "rag-enhanced",
                    "families": ["rag-enhanced"],
                    "parameter_size": "RAG+27B",
                    "quantization_level": "Q4_K_M"
                }
            },
            {
                "name": llm_model_name,
                "model": llm_model_name,
                "modified_at": "2024-12-01T00:00:00.000000000Z",
                "size": 15000000000,
                "digest": f"sha256:{abs(hash(llm_model_name)):064x}",
                "details": {
                    "parent_model": "",
                    "format": "gguf", 
                    "family": "gemma3",
                    "families": ["gemma3"],
                    "parameter_size": "27B",
                    "quantization_level": "Q4_K_M"
                }
            }
        ]
        
        # Ollama í‘œì¤€ ì‘ë‹µ í˜•ì‹
        response = {
            "models": models
        }
        
        print(f"âœ… [TAGS] Ollama ëª¨ë¸ ëª©ë¡ ì‘ë‹µ ìƒì„±: {len(models)}ê°œ ëª¨ë¸")
        for i, model in enumerate(models):
            print(f"   [TAGS] ëª¨ë¸ {i+1}: {model['name']} ({model['details']['parameter_size']})")
        
        return response
        
    except Exception as e:
        print(f"âŒ [TAGS] ëª¨ë¸ ëª©ë¡ ì˜¤ë¥˜: {str(e)}")
        print(f"   [TAGS] ìƒì„¸: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ì‹œì—ë„ Ollama í˜•ì‹ ìœ ì§€
        return {
            "models": []
        }

# ================================
# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ (ìµœì†Œí•œ)
# ================================

@router.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    print(f"ğŸ  [ROOT] ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨")
    return "Ollama is running"