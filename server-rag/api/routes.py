"""
OpenWebUI/Ollama í˜¸í™˜ API ë¼ìš°í„°
"""
import uuid
import time
from typing import Optional
from fastapi import APIRouter, HTTPException, Header, Depends, status, Response
from fastapi.responses import StreamingResponse

from .models import (
    ModelsResponse, WebUIModelsResponse, 
    ChatRequest, ChatResponse, ChatResponseChoice, ChatResponseMessage,
    RetrievalTestRequest, RetrievalTestResponse
)
from .services import model_service
from .auth import get_current_user_optional

# ë¼ìš°í„° ìƒì„±
router = APIRouter()

# ì „ì—­ ì±„íŒ… í•¸ë“¤ëŸ¬ (server.pyì—ì„œ ì„¤ì •)
chat_handler = None

def set_chat_handler(handler):
    """ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • (server.pyì—ì„œ í˜¸ì¶œ)"""
    global chat_handler
    chat_handler = handler
    print(f"âœ… ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")

# ================================
# OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/models", response_model=ModelsResponse)
async def list_openai_models(
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """OpenAI í˜•ì‹ ëª¨ë¸ ëª©ë¡"""
    print(f"ğŸ“‹ GET /api/models ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    models = model_service.get_openai_models()
    print(f"   ë°˜í™˜ëœ ëª¨ë¸ ìˆ˜: {len(models)}")
    
    return ModelsResponse(
        object="list",
        data=models
    )

# ================================
# OpenWebUI/Ollama í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/tags", response_model=WebUIModelsResponse)
async def list_webui_models(
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """Ollama/OpenWebUI í˜•ì‹ ëª¨ë¸ ëª©ë¡"""
    print(f"ğŸ“‹ GET /api/tags ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    models = model_service.get_openwebui_models()
    print(f"   ë°˜í™˜ëœ ëª¨ë¸ ìˆ˜: {len(models)}")
    
    return WebUIModelsResponse(models=models)

@router.get("/api/show")
async def show_model(
    name: str,
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """íŠ¹ì • ëª¨ë¸ ì •ë³´ (Ollama í˜¸í™˜)"""
    print(f"ğŸ“‹ GET /api/show?name={name} ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    model = model_service.get_openwebui_model_by_name(name)
    if not model:
        raise HTTPException(
            status_code=404,
            detail=f"Model '{name}' not found"
        )
    
    print(f"   ëª¨ë¸ ë°œê²¬: {model.name}")
    return model

# ================================
# ì‹œìŠ¤í…œ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/version")
async def get_version():
    """API ë²„ì „ ì •ë³´ (ì¸ì¦ ë¶ˆí•„ìš”)"""
    print(f"ğŸ“‹ GET /api/version ìš”ì²­")
    return {
        "version": "1.0.0",
        "api_version": "1.0",
        "service": "CHEESEADE RAG Server",
        "compatible": ["OpenAI", "Ollama", "OpenWebUI"]
    }

@router.get("/health")
async def health_check(response: Response):
    """ì„œë²„ ìƒíƒœ í™•ì¸ (ì¸ì¦ ë¶ˆí•„ìš”)"""
    print(f"ğŸ“‹ GET /health ìš”ì²­")
    response.status_code = status.HTTP_200_OK
    return {
        "status": "healthy",
        "service": "rag-server",
        "timestamp": int(time.time()),
        "models_available": len(model_service.get_available_models())
    }

# ================================
# ì±„íŒ… ì™„ë£Œ ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ================================

@router.post("/api/chat/completions", response_model=ChatResponse)
async def chat_completions(
    request: ChatRequest,
    authorization: Optional[str] = Header(None)
):
    """OpenAI í˜¸í™˜ ì±„íŒ… ì™„ë£Œ API"""
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    try:
        if authorization:
            print(f"ğŸ”‘ ì¸ì¦: {authorization[:20]}...")
        
        print(f"\nğŸ¯ POST /api/chat/completions")
        print(f"   ëª¨ë¸: {request.model}")
        print(f"   ìŠ¤íŠ¸ë¦¼: {request.stream}")
        
        # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        user_question = request.messages[-1].content
        print(f"   ì§ˆë¬¸: {user_question}")
        
        # ëª¨ë¸ì´ RAG ëª¨ë¸ì¸ì§€ í™•ì¸
        if request.model == chat_handler.rag_model_name:
            # RAG ì‚¬ìš©
            if request.stream:
                return StreamingResponse(
                    chat_handler.stream_rag_response(user_question, request.model),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                response_dict = await chat_handler.handle_chat_request(request)
                return ChatResponse(**response_dict)
        else:
            # ì¼ë°˜ LLM ì‚¬ìš© (Ollamaë¡œ í”„ë¡ì‹œ)
            if request.stream:
                return StreamingResponse(
                    chat_handler.proxy_stream_to_llm(request),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                response_dict = await chat_handler.proxy_to_llm(request)
                return response_dict
        
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/api/chat")
async def chat_simple(request: dict):
    """ê°„ë‹¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„±ìš©)"""
    print(f"ğŸ¯ POST /api/chat (ê°„ë‹¨í•œ í˜•ì‹)")
    
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    # ê°„ë‹¨í•œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if "message" in request:
        # {"message": "ì§ˆë¬¸"} í˜•ì‹
        question = request["message"]
        model = request.get("model", chat_handler.rag_model_name)
        
        chat_request = ChatRequest(
            model=model,
            messages=[{"role": "user", "content": question}],
            stream=request.get("stream", False)
        )
        
        return await chat_completions(chat_request)
    else:
        raise HTTPException(
            status_code=400,
            detail="Invalid request format. Expected 'message' field."
        )

# ================================
# ë””ë²„ê·¸/í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.post("/debug/test-retrieval", response_model=RetrievalTestResponse)
async def test_retrieval(request: RetrievalTestRequest):
    """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    print(f"ğŸ§ª POST /debug/test-retrieval ìš”ì²­: {request.question}")
    
    try:
        result = chat_handler.test_retrieval(request.question)
        return RetrievalTestResponse(**result)
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )

# ================================
# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "CHEESEADE RAG Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "models": "/api/models",
            "tags": "/api/tags", 
            "show": "/api/show?name=<model>",
            "version": "/api/version",
            "health": "/health"
        }
    }