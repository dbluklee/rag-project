"""
OpenWebUI/Ollama í˜¸í™˜ API ë¼ìš°í„°
"""
import uuid
import time
from typing import Optional
from fastapi import APIRouter, HTTPException, Header, Depends, status, Response
from fastapi.responses import StreamingResponse

from .models import (
    ModelsResponse, WebUIModelsResponse, 
    ChatRequest, ChatResponse, ChatResponseChoice, ChatResponseMessage,
    RetrievalTestRequest, RetrievalTestResponse
)
from .services import model_service
from .auth import get_current_user_optional

# ë¼ìš°í„° ìƒì„±
router = APIRouter()

# ì „ì—­ ì±„íŒ… í•¸ë“¤ëŸ¬ (server.pyì—ì„œ ì„¤ì •)
chat_handler = None

def set_chat_handler(handler):
    """ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • (server.pyì—ì„œ í˜¸ì¶œ)"""
    global chat_handler
    chat_handler = handler
    print(f"âœ… ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")

# ================================
# OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/models", response_model=ModelsResponse)
async def list_openai_models(
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """OpenAI í˜•ì‹ ëª¨ë¸ ëª©ë¡"""
    print(f"ğŸ“‹ GET /api/models ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    models = model_service.get_openai_models()
    print(f"   ë°˜í™˜ëœ ëª¨ë¸ ìˆ˜: {len(models)}")
    
    return ModelsResponse(
        object="list",
        data=models
    )

# ================================
# OpenWebUI/Ollama í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/tags", response_model=WebUIModelsResponse)
async def list_webui_models(
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """Ollama/OpenWebUI í˜•ì‹ ëª¨ë¸ ëª©ë¡"""
    print(f"ğŸ“‹ GET /api/tags ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    models = model_service.get_openwebui_models()
    print(f"   ë°˜í™˜ëœ ëª¨ë¸ ìˆ˜: {len(models)}")
    
    return WebUIModelsResponse(models=models)

@router.get("/api/show")
async def show_model(
    name: str,
    current_user: Optional[dict] = Depends(get_current_user_optional)
):
    """íŠ¹ì • ëª¨ë¸ ì •ë³´ (Ollama í˜¸í™˜)"""
    print(f"ğŸ“‹ GET /api/show?name={name} ìš”ì²­")
    if current_user:
        print(f"   ì‚¬ìš©ì: {current_user.get('name', 'Unknown')}")
    
    model = model_service.get_openwebui_model_by_name(name)
    if not model:
        raise HTTPException(
            status_code=404,
            detail=f"Model '{name}' not found"
        )
    
    print(f"   ëª¨ë¸ ë°œê²¬: {model.name}")
    return model

# ================================
# ì‹œìŠ¤í…œ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/api/version")
async def get_version():
    """API ë²„ì „ ì •ë³´ (ì¸ì¦ ë¶ˆí•„ìš”)"""
    print(f"ğŸ“‹ GET /api/version ìš”ì²­")
    return {
        "version": "1.0.0",
        "api_version": "1.0",
        "service": "CHEESEADE RAG Server",
        "compatible": ["OpenAI", "Ollama", "OpenWebUI"]
    }

@router.get("/health")
async def health_check(response: Response):
    """ì„œë²„ ìƒíƒœ í™•ì¸ (ì¸ì¦ ë¶ˆí•„ìš”)"""
    print(f"ğŸ“‹ GET /health ìš”ì²­")
    response.status_code = status.HTTP_200_OK
    return {
        "status": "healthy",
        "service": "rag-server",
        "timestamp": int(time.time()),
        "models_available": len(model_service.get_available_models())
    }

# ================================
# ì±„íŒ… ì™„ë£Œ ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ================================

@router.post("/api/chat/completions", response_model=ChatResponse)
async def chat_completions(
    request: ChatRequest,
    authorization: Optional[str] = Header(None)
):
    """OpenAI í˜¸í™˜ ì±„íŒ… ì™„ë£Œ API"""
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    try:
        if authorization:
            print(f"ğŸ”‘ ì¸ì¦: {authorization[:20]}...")
        
        print(f"\nğŸ¯ POST /api/chat/completions")
        print(f"   ëª¨ë¸: {request.model}")
        print(f"   ìŠ¤íŠ¸ë¦¼: {request.stream}")
        
        # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        user_question = request.messages[-1].content
        print(f"   ì§ˆë¬¸: {user_question}")
        
        # ëª¨ë¸ì´ RAG ëª¨ë¸ì¸ì§€ í™•ì¸
        if request.model == chat_handler.rag_model_name:
            # RAG ì‚¬ìš©
            if request.stream:
                return StreamingResponse(
                    chat_handler.stream_rag_response(user_question, request.model),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                response_dict = await chat_handler.handle_chat_request(request)
                return ChatResponse(**response_dict)
        else:
            # ì¼ë°˜ LLM ì‚¬ìš© (Ollamaë¡œ í”„ë¡ì‹œ)
            if request.stream:
                return StreamingResponse(
                    chat_handler.proxy_stream_to_llm(request),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                response_dict = await chat_handler.proxy_to_llm(request)
                return response_dict
        
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/api/chat")
async def chat_simple(request: dict):
    """ê°„ë‹¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Ollama í˜¸í™˜) - ì˜¬ë°”ë¥¸ í˜•ì‹ ì‘ë‹µ"""
    print(f"ğŸ¯ POST /api/chat (Ollama í˜•ì‹)")
    
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    try:
        # ê°„ë‹¨í•œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if "message" in request:
            # {"message": "ì§ˆë¬¸"} í˜•ì‹
            question = request["message"]
            model = request.get("model", chat_handler.rag_model_name)
            stream = request.get("stream", False)
        elif "messages" in request:
            # {"messages": [...]} í˜•ì‹ (OpenWebUI í‘œì¤€)
            question = request["messages"][-1]["content"]
            model = request.get("model", chat_handler.rag_model_name)
            stream = request.get("stream", False)
        else:
            raise HTTPException(
                status_code=400,
                detail="Invalid request format. Expected 'message' or 'messages' field."
            )
        
        print(f"   ëª¨ë¸: {model}")
        print(f"   ì§ˆë¬¸: {question}")
        print(f"   ìŠ¤íŠ¸ë¦¼: {stream}")
        
        # ChatRequest ê°ì²´ ìƒì„±
        chat_request = ChatRequest(
            model=model,
            messages=[{"role": "user", "content": question}],
            stream=stream
        )
        
        # chat_completionsë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì§ì ‘ chat_handler ì‚¬ìš©
        if chat_request.model == chat_handler.rag_model_name:
            # RAG ëª¨ë¸ ì‚¬ìš©
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (Ollama í˜•ì‹)
                async def ollama_stream_generator():
                    async for chunk in chat_handler.stream_rag_response(question, chat_request.model):
                        # OpenAI í˜•ì‹ì„ Ollama í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        if chunk.startswith("data: "):
                            chunk_data = chunk[6:].strip()
                            if chunk_data == "[DONE]":
                                break
                            try:
                                parsed = json.loads(chunk_data)
                                if "choices" in parsed and parsed["choices"]:
                                    content = parsed["choices"][0].get("delta", {}).get("content", "")
                                    if content:
                                        ollama_chunk = {
                                            "model": chat_request.model,
                                            "created_at": "2024-01-01T00:00:00Z",
                                            "message": {
                                                "role": "assistant",
                                                "content": content
                                            },
                                            "done": False
                                        }
                                        yield f"data: {json.dumps(ollama_chunk)}\n\n"
                            except json.JSONDecodeError:
                                continue
                    
                    # ì¢…ë£Œ ì‹ í˜¸
                    final_chunk = {
                        "model": chat_request.model,
                        "created_at": "2024-01-01T00:00:00Z",
                        "message": {
                            "role": "assistant",
                            "content": ""
                        },
                        "done": True
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                
                return StreamingResponse(
                    ollama_stream_generator(),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                # ë…¼ìŠ¤íŠ¸ë¦¬ë° RAG ì‘ë‹µ (Ollama í˜•ì‹)
                response_content = await chat_handler.process_with_rag(question)
                
                # Ollama í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
                return {
                    "model": chat_request.model,
                    "created_at": "2024-01-01T00:00:00Z",
                    "message": {
                        "role": "assistant",
                        "content": response_content
                    },
                    "done": True,
                    "total_duration": 1000000000,  # 1ì´ˆ (ë‚˜ë…¸ì´ˆ)
                    "load_duration": 100000000,
                    "prompt_eval_count": 10,
                    "prompt_eval_duration": 200000000,
                    "eval_count": 20,
                    "eval_duration": 500000000
                }
        else:
            # ì¼ë°˜ LLM ëª¨ë¸ - LLM ì„œë²„ë¡œ í”„ë¡ì‹œ
            print(f"ğŸ”„ LLM ì„œë²„ë¡œ í”„ë¡ì‹œ: {model}")
            
            # Ollama ì„œë²„ë¡œ ì§ì ‘ í”„ë¡ì‹œ
            ollama_request = {
                "model": model,
                "messages": [{"role": "user", "content": question}],
                "stream": stream
            }
            
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° í”„ë¡ì‹œ
                async def proxy_stream():
                    try:
                        import aiohttp
                        async with aiohttp.ClientSession() as session:
                            async with session.post(
                                f"{chat_handler.llm_server_url}/api/chat",
                                json=ollama_request
                            ) as response:
                                async for chunk in response.content.iter_chunked(1024):
                                    yield chunk
                    except Exception as e:
                        print(f"âŒ í”„ë¡ì‹œ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
                        error_chunk = {
                            "model": model,
                            "created_at": "2024-01-01T00:00:00Z",
                            "message": {
                                "role": "assistant", 
                                "content": f"Error: {str(e)}"
                            },
                            "done": True
                        }
                        yield f"data: {json.dumps(error_chunk)}\n\n".encode()
                
                return StreamingResponse(
                    proxy_stream(),
                    media_type="text/plain",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    }
                )
            else:
                # ë…¼ìŠ¤íŠ¸ë¦¬ë° í”„ë¡ì‹œ
                try:
                    proxy_response = requests.post(
                        f"{chat_handler.llm_server_url}/api/chat",
                        json=ollama_request,
                        timeout=120
                    )
                    
                    if proxy_response.status_code == 200:
                        return proxy_response.json()
                    else:
                        # ì—ëŸ¬ë¥¼ Ollama í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
                        return {
                            "model": model,
                            "created_at": "2024-01-01T00:00:00Z",
                            "message": {
                                "role": "assistant",
                                "content": f"LLM server error: {proxy_response.status_code}"
                            },
                            "done": True
                        }
                        
                except requests.exceptions.RequestException as e:
                    print(f"âŒ í”„ë¡ì‹œ ìš”ì²­ ì˜¤ë¥˜: {e}")
                    return {
                        "model": model,
                        "created_at": "2024-01-01T00:00:00Z",
                        "message": {
                            "role": "assistant",
                            "content": f"Connection error: {str(e)}"
                        },
                        "done": True
                    }
            
    except Exception as e:
        print(f"âŒ chat_simple ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ë„ Ollama í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return {
            "model": request.get("model", "unknown"),
            "created_at": "2024-01-01T00:00:00Z",
            "message": {
                "role": "assistant",
                "content": f"Error: {str(e)}"
            },
            "done": True
        }

# ================================
# ë””ë²„ê·¸/í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.post("/debug/test-retrieval", response_model=RetrievalTestResponse)
async def test_retrieval(request: RetrievalTestRequest):
    """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    if not chat_handler:
        raise HTTPException(
            status_code=503,
            detail="Chat handler not initialized"
        )
    
    print(f"ğŸ§ª POST /debug/test-retrieval ìš”ì²­: {request.question}")
    
    try:
        result = chat_handler.test_retrieval(request.question)
        return RetrievalTestResponse(**result)
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )

# ================================
# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ================================

@router.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "CHEESEADE RAG Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "models": "/api/models",
            "tags": "/api/tags", 
            "show": "/api/show?name=<model>",
            "version": "/api/version",
            "health": "/health"
        }
    }