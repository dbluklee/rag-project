import torch
from langchain_huggingface import HuggingFaceEmbeddings
import os

def get_bge_m3_model() -> HuggingFaceEmbeddings:
    """
    BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    USE_CUDA í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ CPU/GPUë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    # USE_CUDA í™˜ê²½ë³€ìˆ˜ í™•ì¸
    use_cuda = os.getenv('USE_CUDA', 'true').lower() == 'true'
    
    # USE_CUDAê°€ falseë©´ CUDA_VISIBLE_DEVICESë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •í•˜ì—¬ GPU ë¹„í™œì„±í™”
    if not use_cuda:
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸ”§ USE_CUDA=false - GPU ë¹„í™œì„±í™”, CPU ëª¨ë“œë¡œ ì „í™˜")
        device = 'cpu'
    elif torch.cuda.is_available():
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_free = torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)
            gpu_memory_gb = gpu_memory / (1024**3)
            gpu_free_gb = gpu_free / (1024**3)
            
            print(f"ğŸ” GPU ì „ì²´ ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB")
            print(f"ğŸ” GPU ì—¬ìœ  ë©”ëª¨ë¦¬: {gpu_free_gb:.1f}GB")
            
            # ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ 2GB ë¯¸ë§Œì´ë©´ ì—ëŸ¬ ë°œìƒ
            if gpu_free_gb < 2.0:
                raise RuntimeError(
                    f"âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ì—¬ìœ  ë©”ëª¨ë¦¬: {gpu_free_gb:.1f}GB < 2.0GB í•„ìš”\n"
                    f"í•´ê²°ë°©ë²•:\n"
                    f"1. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n"
                    f"2. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©\n"
                    f"3. ë” í° GPU ë©”ëª¨ë¦¬ í™˜ê²½ ì‚¬ìš©"
                )
            else:
                device = 'cuda'
        except Exception as e:
            # GPU ìƒíƒœ í™•ì¸ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ CPUë¡œ ì „í™˜
            if "CUDA" in str(e) or "GPU" in str(e):
                raise RuntimeError(
                    f"âŒ GPU í™•ì¸ ì‹¤íŒ¨: {e}\n"
                    f"í•´ê²°ë°©ë²•:\n"
                    f"1. NVIDIA ë“œë¼ì´ë²„ í™•ì¸\n" 
                    f"2. CUDA ì„¤ì¹˜ í™•ì¸\n"
                    f"3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©"
                )
            else:
                raise e
    else:
        raise RuntimeError(
            "âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\n"
            "í•´ê²°ë°©ë²•:\n"
            "1. NVIDIA GPU ë° ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸\n"
            "2. CUDA ì„¤ì¹˜ í™•ì¸\n" 
            "3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©"
        )
    
    print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë””ë°”ì´ìŠ¤: {device}")
    
    model_name = 'BAAI/bge-m3'
    model_kwargs = {'device': device}
    encode_kwargs = {'normalize_embeddings': True}
    
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({device})")
        return embeddings
    except Exception as e:
        raise RuntimeError(
            f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\n"
            f"í•´ê²°ë°©ë²•:\n"
            f"1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)\n"
            f"2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸\n"
            f"3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‹œë„"
        )