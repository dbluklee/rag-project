"""
RAGAS í‰ê°€ ì‹¤í–‰ ëª¨ë“ˆ - RAG ì„œë²„ ê²°ê³¼ ê¸°ë°˜ í‰ê°€
RAG ì„œë²„ì˜ ì²­í‚¹/ì„ë² ë”©/ë¦¬íŠ¸ë¦¬ë²„ ë³€ê²½ì‚¬í•­ì´ ìë™ìœ¼ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤
"""

import pandas as pd
from typing import Dict, Any, List
from datasets import Dataset
import logging
from datetime import datetime

# RAGAS imports
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    context_relevancy,
    answer_similarity,
    answer_correctness
)

class RAGASRunner:
    """RAGAS í‰ê°€ ì‹¤í–‰ í´ë˜ìŠ¤ - RAG ì„œë²„ ê²°ê³¼ë§Œ ì‚¬ìš©"""
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        self.config = config
        self.logger = logger
        
        # RAGAS ë©”íŠ¸ë¦­ ë§¤í•‘
        self.metric_map = {
            'faithfulness': faithfulness,
            'answer_relevancy': answer_relevancy,
            'context_precision': context_precision,
            'context_recall': context_recall,
            'context_relevancy': context_relevancy,
            'answer_similarity': answer_similarity,
            'answer_correctness': answer_correctness
        }
        
        # ì„¤ì •ëœ ë©”íŠ¸ë¦­ë§Œ ì„ íƒ
        self.selected_metrics = []
        for metric_name in self.config['ragas_metrics']:
            if metric_name in self.metric_map:
                self.selected_metrics.append(self.metric_map[metric_name])
                self.logger.info(f"ğŸ“Š ë©”íŠ¸ë¦­ ì¶”ê°€: {metric_name}")
        
        self.logger.info(f"ğŸ¯ ì´ {len(self.selected_metrics)}ê°œ ë©”íŠ¸ë¦­ ì„ íƒë¨")
    
    def run_evaluation(self, dataset: Dataset) -> Dict[str, Any]:
        """RAGAS í‰ê°€ ì‹¤í–‰ - RAG ì„œë²„ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€"""
        self.logger.info("ğŸš€ RAGAS í‰ê°€ ì‹œì‘...")
        self.logger.info("ğŸ“‹ RAG ì„œë²„ì—ì„œ ì œê³µëœ ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€")
        self.logger.info("ğŸ”„ RAG ì„œë²„ì˜ ì²­í‚¹/ì„ë² ë”©/ë¦¬íŠ¸ë¦¬ë²„ ë³€ê²½ì‚¬í•­ ìë™ ë°˜ì˜ë¨")
        
        try:
            # ë‚´ë¶€ LLM ì„œë²„ë§Œ í‰ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©
            evaluator_model = self.config['models'].get('evaluator_model', 'gemma3:27b-it-q4_K_M')
            llm_server_url = self.config['servers']['llm_server_url']
            
            self.logger.info(f"ğŸ¤– í‰ê°€ìš© LLM: {evaluator_model} (ë‚´ë¶€ LLM ì„œë²„)")
            self.logger.info(f"ğŸ“Š ë°ì´í„°: RAG ì„œë²„ ì œê³µ (answer + contexts)")
            self.logger.info(f"ğŸ”§ ì„ë² ë”©/ë¦¬íŠ¸ë¦¬ë²„: RAG ì„œë²„ ë‚´ë¶€ (RAGAS ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”)")
            
            # Langchain Ollama LLM ì„¤ì • (í‰ê°€ìš©ë§Œ)
            from langchain_ollama import ChatOllama
            
            eval_llm = ChatOllama(
                model=evaluator_model,
                base_url=llm_server_url,
                timeout=120,
                temperature=0.1  # í‰ê°€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì„¤ì •
            )
            
            # RAGAS í‰ê°€ ì‹¤í–‰ - ì„ë² ë”©/ë¦¬íŠ¸ë¦¬ë²„ ì—†ì´ ê²°ê³¼ë§Œ í‰ê°€
            start_time = datetime.now()
            self.logger.info(f"â³ í‰ê°€ ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # RAG ì„œë²„ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ë¶ˆí•„ìš”)
            results = evaluate(
                dataset=dataset,
                metrics=self.selected_metrics,
                llm=eval_llm  # í‰ê°€ìš© LLMë§Œ ì‚¬ìš©, ì„ë² ë”©ì€ ê¸°ë³¸ê°’ ì‚¬ìš©
            )
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.logger.info(f"âœ… RAGAS í‰ê°€ ì™„ë£Œ")
            self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            self.logger.info(f"ğŸ“Š í‰ê°€ëœ í•­ëª© ìˆ˜: {len(dataset)}")
            self.logger.info(f"ğŸ¯ RAG ì„œë²„ êµ¬ì„± ë³€ê²½ì‹œ ì¬í‰ê°€ë§Œ í•˜ë©´ ë¨")
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            processed_results = self._process_results(results, duration)
            
            return processed_results
            
        except Exception as e:
            self.logger.error(f"âŒ RAGAS í‰ê°€ ì‹¤íŒ¨: {e}")
            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ë¡œê¹…
            import traceback
            self.logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise
    
    def _process_results(self, raw_results: Dict[str, Any], duration) -> Dict[str, Any]:
        """RAGAS ê²°ê³¼ í›„ì²˜ë¦¬"""
        self.logger.info("ğŸ”„ í‰ê°€ ê²°ê³¼ í›„ì²˜ë¦¬ ì¤‘...")
        
        processed = {
            'timestamp': datetime.now().isoformat(),
            'duration_seconds': duration.total_seconds(),
            'evaluation_summary': {},
            'detailed_metrics': {},
            'performance_analysis': {}
        }
        
        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì¶”ì¶œ
        for metric_name in self.config['ragas_metrics']:
            if metric_name in raw_results:
                score = raw_results[metric_name]
                processed['evaluation_summary'][metric_name] = float(score)
                
                # ì„ê³„ê°’ê³¼ ë¹„êµ
                threshold = self.config['thresholds'].get(metric_name, 0.5)
                performance = "PASS" if score >= threshold else "FAIL"
                
                processed['detailed_metrics'][metric_name] = {
                    'score': float(score),
                    'threshold': threshold,
                    'performance': performance,
                    'gap': float(score - threshold)
                }
                
                self.logger.info(f"ğŸ“ˆ {metric_name}: {score:.4f} ({performance})")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        scores = list(processed['evaluation_summary'].values())
        if scores:
            overall_score = sum(scores) / len(scores)
            processed['evaluation_summary']['overall_score'] = overall_score
            
            overall_threshold = self.config['thresholds'].get('overall_score', 0.75)
            overall_performance = "PASS" if overall_score >= overall_threshold else "FAIL"
            
            processed['performance_analysis'] = {
                'overall_score': overall_score,
                'overall_threshold': overall_threshold,
                'overall_performance': overall_performance,
                'total_metrics': len(scores),
                'passed_metrics': sum(1 for m in processed['detailed_metrics'].values() if m['performance'] == 'PASS'),
                'failed_metrics': sum(1 for m in processed['detailed_metrics'].values() if m['performance'] == 'FAIL')
            }
            
            self.logger.info(f"ğŸ¯ ì „ì²´ ì ìˆ˜: {overall_score:.4f} ({overall_performance})")
            self.logger.info(f"ğŸ“Š í†µê³¼/ì‹¤íŒ¨: {processed['performance_analysis']['passed_metrics']}/{processed['performance_analysis']['failed_metrics']}")
        
        return processed
    
    def analyze_by_category(self, dataset: Dataset, results: Dict[str, Any], 
                          original_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„"""
        self.logger.info("ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ê·¸ë£¹í™”
            categories = {}
            for i, item in enumerate(dataset):
                # ì›ë³¸ ê²°ê³¼ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                if i < len(original_results):
                    category = original_results[i].get('category', 'Unknown')
                    if category not in categories:
                        categories[category] = {
                            'indices': [],
                            'questions': [],
                            'answers': [],
                            'ground_truths': []
                        }
                    
                    categories[category]['indices'].append(i)
                    categories[category]['questions'].append(item['question'])
                    categories[category]['answers'].append(item['answer'])
                    categories[category]['ground_truths'].append(item['ground_truth'])
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ê³„ì‚°
            category_analysis = {}
            
            for category, data in categories.items():
                category_metrics = {}
                
                # ê° ë©”íŠ¸ë¦­ë³„ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê³„ì‚°
                for metric_name in self.config['ragas_metrics']:
                    if metric_name in results['evaluation_summary']:
                        # ì „ì²´ í‰ê· ì„ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì¹´í…Œê³ ë¦¬ë³„ ì¬í‰ê°€ í•„ìš”)
                        category_metrics[metric_name] = results['evaluation_summary'][metric_name]
                
                # ì¹´í…Œê³ ë¦¬ ì „ì²´ ì ìˆ˜
                if category_metrics:
                    category_score = sum(category_metrics.values()) / len(category_metrics)
                else:
                    category_score = 0.0
                
                category_analysis[category] = {
                    'count': len(data['indices']),
                    'metrics': category_metrics,
                    'overall_score': category_score,
                    'sample_questions': data['questions'][:3]  # ìƒ˜í”Œ ì§ˆë¬¸ 3ê°œ
                }
                
                self.logger.info(f"ğŸ“‚ {category}: {len(data['indices'])}ê°œ ì§ˆë¬¸, ì ìˆ˜: {category_score:.4f}")
            
            return category_analysis
            
        except Exception as e:
            self.logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def generate_recommendations(self, results: Dict[str, Any]) -> Dict[str, List[str]]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        self.logger.info("ğŸ’¡ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
        
        recommendations = {
            'high_priority': [],
            'medium_priority': [],
            'low_priority': [],
            'general_tips': []
        }
        
        # ë©”íŠ¸ë¦­ë³„ ê¶Œì¥ì‚¬í•­
        for metric_name, metric_data in results.get('detailed_metrics', {}).items():
            score = metric_data['score']
            threshold = metric_data['threshold']
            
            if score < threshold:
                gap = threshold - score
                
                if metric_name == 'faithfulness' and gap > 0.2:
                    recommendations['high_priority'].append(
                        f"Faithfulness ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ ({score:.3f}). "
                        "RAG ì„œë²„ì˜ ê²€ìƒ‰ëœ ë¬¸ì„œ í’ˆì§ˆì„ ê°œì„ í•˜ê³ , ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ì„œ "
                        "ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ê°•ì¡°í•˜ì„¸ìš”."
                    )
                
                elif metric_name == 'answer_relevancy' and gap > 0.15:
                    recommendations['high_priority'].append(
                        f"Answer Relevancy ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ ({score:.3f}). "
                        "RAG ì„œë²„ì˜ ì§ˆë¬¸ ì´í•´ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ê°œì„ í•˜ì„¸ìš”."
                    )
                
                elif metric_name == 'context_precision' and gap > 0.15:
                    recommendations['medium_priority'].append(
                        f"Context Precisionì´ ë‚®ìŠµë‹ˆë‹¤ ({score:.3f}). "
                        "RAG ì„œë²„ì˜ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ íŠœë‹ì´ë‚˜ ì¸ë±ìŠ¤ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
                    )
                
                elif metric_name == 'context_recall' and gap > 0.15:
                    recommendations['medium_priority'].append(
                        f"Context Recallì´ ë‚®ìŠµë‹ˆë‹¤ ({score:.3f}). "
                        "RAG ì„œë²„ì˜ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì²­í‚¹ ì „ëµì„ ì¬ê²€í† í•˜ì„¸ìš”."
                    )
                
                else:
                    recommendations['low_priority'].append(
                        f"{metric_name} ì ìˆ˜ ê°œì„  í•„ìš”: {score:.3f} -> {threshold:.3f}"
                    )
        
        # ì „ì²´ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        overall_score = results.get('evaluation_summary', {}).get('overall_score', 0)
        if overall_score < 0.6:
            recommendations['high_priority'].append(
                "ì „ë°˜ì ì¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. RAG ì„œë²„ ì•„í‚¤í…ì²˜ ì „ë°˜ì˜ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )
        elif overall_score < 0.75:
            recommendations['medium_priority'].append(
                "ì„±ëŠ¥ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤. RAG ì„œë²„ì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ê°œì„ í•˜ì„¸ìš”."
            )
        
        # RAG ì„œë²„ êµ¬ì„±ìš”ì†Œë³„ ê°œì„  íŒ
        recommendations['general_tips'] = [
            "RAG ì„œë²„: ë¬¸ì„œ ì²­í‚¹ í¬ê¸°ì™€ ê²¹ì¹¨ ë¹„ìœ¨ ìµœì í™”",
            "RAG ì„œë²„: ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹",
            "RAG ì„œë²„: ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (top_k, similarity_threshold) íŠœë‹",
            "RAG ì„œë²„: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸",
            "RAG ì„œë²„: ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ground truth ë³´ì™„"
        ]
        
        # ê¶Œì¥ì‚¬í•­ ìˆ˜ ë¡œê¹…
        total_recommendations = (
            len(recommendations['high_priority']) + 
            len(recommendations['medium_priority']) + 
            len(recommendations['low_priority'])
        )
        
        self.logger.info(f"ğŸ’¡ ì´ {total_recommendations}ê°œ ê¶Œì¥ì‚¬í•­ ìƒì„±")
        self.logger.info(f"   ğŸ”´ ë†’ìŒ: {len(recommendations['high_priority'])}ê°œ")
        self.logger.info(f"   ğŸŸ¡ ì¤‘ê°„: {len(recommendations['medium_priority'])}ê°œ") 
        self.logger.info(f"   ğŸŸ¢ ë‚®ìŒ: {len(recommendations['low_priority'])}ê°œ")
        
        return recommendations